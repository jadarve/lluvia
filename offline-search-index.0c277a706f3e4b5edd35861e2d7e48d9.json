[{"body":"Buffers are unstructured regions of contiguous memory. Buffers are created from Memory objects:\nPython  C++   1 2 3 4 5 6 7 8 9 10 11  import lluvia as ll  session = ll.createSession()  hostMemory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal,  ll.MemoryPropertyFlagBits.HostVisible,  ll.MemoryPropertyFlagBits.HostCoherent])  aBuffer = hostMemory.createBuffer(1024, usageFlags=[ll.BufferUsageFlagBits.TransferDst,  ll.BufferUsageFlagBits.TransferSrc,  ll.BufferUsageFlagBits.StorageBuffer])   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include \u003ciostream\u003e#include \u003cvulkan/vulkan.hpp\u003e#include \"lluvia/core.h\" #include \u003cvulkan/vulkan.hpp\u003e int main() {   ll::SessionDescriptor desc = ll::SessionDescriptor().enableDebug(true);   std::shared_ptr\u003cll::Session\u003e session = ll::Session::create(desc);   hostMemory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal,  ll.MemoryPropertyFlagBits.HostVisible,  ll.MemoryPropertyFlagBits.HostCoherent])   const auto usageFlags = vk::BufferUsageFlags { vk::BufferUsageFlagBits::eStorageBuffer  | vk::BufferUsageFlagBits::eTransferSrc  | vk::BufferUsageFlagBits::eTransferDst};   auto aBuffer = hostMemory-\u003ecreateBuffer(1024, usageFlags); }    The first parameter is the requested size in bytes. The usageFlags indicated the intended usage of this buffer; the values are taken directly from the Vulkan BufferUsageFlagBits. The most used values are:\n   Flag Description     StorageBuffer Indicates that the buffer is going to be used for general storage.   TransferDst Indicates that the buffer can be used as destination for transfer commands.   TransferSrc Indicates that the buffer can be used as source for transfer commands.    ","categories":"","description":"","excerpt":"Buffers are unstructured regions of contiguous memory. Buffers are …","ref":"/docs/reference/objects/buffer/","tags":"","title":"Buffer"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/node_system/compute_node/","tags":"","title":"Compute nodes"},{"body":"   ","categories":"","description":"","excerpt":"   ","ref":"/docs/examples/1-optical_flow/","tags":"","title":"Real-time optical flow"},{"body":"A Session is the main object in a Lluvia application. It holds the references to the underlying device used for computation. To see the available devices, run:\nPython  C++   1 2 3 4  import lluvia as ll  for device in ll.getAvailableDevices():  print(device)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #include \u003clluvia/core.h\u003e #include \u003ciostream\u003e int main() {   const auto availableDevices = ll::Session::getAvailableDevices();   for(auto deviceDesc : availableDevices) {   std::cout \u003c\u003c \"ID: \" \u003c\u003c deviceDesc.id  \u003c\u003c \" type: \" \u003c\u003c ll::deviceTypeToString(std::forward\u003cll::DeviceType\u003e(deviceDesc.deviceType))  \u003c\u003c \" name: \" \u003c\u003c deviceDesc.name \u003c\u003c std::endl;  }   return 0; }    and the output can look like:\nid: 7040 type: DiscreteGPU name: GeForce GTX 1080 id: 0 type: CPU name: llvmpipe (LLVM 12.0.0, 256 bits) id: 1042 type: IntegratedGPU name: Intel(R) HD Graphics 4600 (HSW GT2) To create a session:\nPython  C++   1 2 3 4 5 6 7 8  import lluvia as ll  devices = ll.getAvailableDevices()  # ... select the device appropriate to your needs selectedDevice = devices[0]  session = ll.createSession(enableDebug=True, device=selectedDevice)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #include \u003clluvia/core.h\u003e #include \u003cmemory\u003e int main() {   const auto availableDevices = ll::Session::getAvailableDevices();   // ... select the device appropriate to your needs  auto selectedDevice = availableDevices[0];   auto desc = ll::SessionDescriptor()  .enableDebug(true)  .setDeviceDescriptor(selectedDevice);   std::shared_ptr\u003cll::Session\u003e session = ll::Session::create(desc);   return 0; }    Note If no device is passed during the creation of a Session, the default behavior is to select the first device from the list of available ones.  The enableDebug flag enables the Vulkan validation layers for receiving messages about bad usage of the API. This can be useful while building your compute pipelines, but should be disabled in Production for reducing the communication overhead with the GPU.\nSeveral object types are creating from a session, among the most important are:\ngraph Session --\u003e Memory Session --\u003e Program Session --\u003e CommandBuffer Session --\u003e Duration Session --\u003e ComputeNode Session --\u003e ContainerNode What’s next Check the Memory page to know about the different memory types in Lluvia.\n","categories":"","description":"","excerpt":"A Session is the main object in a Lluvia application. It holds the …","ref":"/docs/reference/session/","tags":"","title":"Session"},{"body":"Dependencies   Install the following packages in your system if they not available yet:\n1 2 3  sudo apt install \\  build-essential \\  clang     Bazel:\n1 2 3 4  curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list sudo apt update sudo apt install bazel     LunarG Vulkan SDK:\n1 2 3 4  wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo apt-key add - sudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-1.3.211-focal.list https://packages.lunarg.com/vulkan/1.3.211/lunarg-vulkan-1.3.211-focal.list sudo apt update sudo apt install vulkan-sdk   Verify that the SDK was successfully installed by running:\n1  vulkaninfo     Python3 dependencies\n1  sudo apt install python3-pip     Build C++ Libraries Clone and compile Lluvia’s C++ libraries:\n1 2 3 4 5 6 7  git clone https://github.com/jadarve/lluvia.git cd lluvia  # install requirements listed in lluvia's root folder sudo pip3 install -r requirements.txt  bazel build //lluvia/cpp/...   Run the tests to verify that your compilation runs properly:\n1  bazel test //lluvia/cpp/...   Python3 package To build the Python3 package, execute the commands below from the repository’s top-level directory. You can create a virtual environment to isolate the installation:\n1 2  bazel build //lluvia/python:lluvia_wheel pip3 install bazel-bin/lluvia/python/lluvia-0.0.1-py3-none-any.whl   Open a Python3 interpreter and import lluvia:\n1 2 3  import lluvia as ll  session = ll.createSession()   If the import completes successfully, lluvia is ready to use.\n","categories":"","description":"","excerpt":"Dependencies   Install the following packages in your system if they …","ref":"/docs/gettingstarted/installation/linux/","tags":"","title":"Ubuntu"},{"body":"Designing and developing real-time Computer Vision algorithms is a difficult task. On one hand, one needs to formulate the algorithm mathematically in such a way that enables fast calculations on modern compute hardware. On the other hand, such mathematical formulation has to be transformed into machine code, optimized and tested to satisfy system requirements such as hardware usage and frame rate. Accomplishing both tasks successfully is the job of research and engineering teams with highly specialized knowledge.\nLluvia has been designed around the idea of reducing the effort for designing and implementing Computer Vision algorithms for real-time applications. The engine is built on top the Vulkan graphics and compute API. By using Vulkan, it is possible to run the algorithms on any modern GPU that supports the API. The core libraries are coded in C++ and can be compiled for several operating systems, currently Linux and Android. Wrappers for high-level languages such as Python are maintained as well.\nWorkflow Lluvia uses a compute graph to organize and schedule computations on the GPU. The development workflow circles around coding and debugging nodes in such a graph until the whole algorithm is built:\n  The node’s inputs, outputs and parameters are described in a Lua script . This description will later be used to instantiate nodes in the graph.\n  The node’s computation in the GPU is coded as a compute shader in Open GL Shading Language (GLSL). Shaders are compiled into SPIR-V intemediate representation for later load into the GPU.\n  The node’s description and compute shader are loaded into Lluvia’s runtime. After this, nodes can be instantiated to build the compute graph and be dispatched to the GPU.\n  From a user perspective, one needs to only care about describing nodes (inputs, outputs, compute shader, etc.) and connecting nodes to form a graph. Lluvia takes care of the low-level details of dispatching the graph for execution onto the GPU. This workflow allows porting the compute graph from one platform to another with ease.\nCheck the Getting Started guides for examples on how to describe computations in Lluvia.\nAlternatives There are many other alternatives to use for coding and deploying Computer Vision algorithms. The list below is by no means an exhaustive review. Please contact me if you want other frameworks to be included.\n  OpenCV The go-to alternative for fast prototyping and deployment of CV algorithms. OpenCV is a mature project that can be used on many platforms (Linux, OSX, Windows, Android). It contains a bast library of algorithms, some of them with GPU implementations.\n  Halide is a programming language for coding high-performance image processing algorithms. The language is embedded in C++ and can dispatch execution of the algorithms to CPUs and GPUs depending on the available hardware.\n  Mediapipe A framework for developing complex Computer Vision pipelines combining several frameworks such as OpenCV, TensorFlow, TFLite.\n  ","categories":"","description":"","excerpt":"Designing and developing real-time Computer Vision algorithms is a …","ref":"/docs/about/","tags":"","title":"About"},{"body":"Install docker following the official documentation and the post installation guide.\nClone lluvia and build the container\n1 2 3 4  git clone https://github.com/jadarve/lluvia.git cd lluvia  docker build ci/ --tag=\"lluvia:local\"   Run the container, mounting lluvia’s repository at /lluvia:\n1 2 3  docker run \\  --mount type=bind,source=\"$(pwd)\",target=/lluvia \\  -it --rm \"lluvia:local\" /bin/bash   Inside the container, build and test all of lluvia package:\n1 2 3  cd lluvia bazel build //... bazel test --test_output=errors //...   ","categories":"","description":"","excerpt":"Install docker following the official documentation and the post …","ref":"/docs/gettingstarted/installation/linux_docker/","tags":"","title":"Linux using docker"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/node_system/container_node/","tags":"","title":"Container nodes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/objects/image/","tags":"","title":"Image"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/objects/image_view/","tags":"","title":"ImageView"},{"body":"Memory objects represent regions of memory that can be used to allocate objects. Lluvia uses the memory types defined by the Vulkan API. You may also refer to this article by Adam Sawicki on how memory is offered by different GPU vendors.\nMemory types Memory objects are created from a Lluvia Session. The code block below enumerate the available memory options:\nPython  C++   1 2 3 4 5 6 7 8  import lluvia as ll  session = ll.createSession(enableDebug=True)  for n, memflags in enumerate(session.getSupportedMemoryPropertyFlags()):  print('Memory index:', n)  print(' supported flags:', [p.name for p in memflags])  print()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #include \u003ciostream\u003e#include \"lluvia/core.h\" #include \u003cvulkan/vulkan.hpp\u003e int main() {   ll::SessionDescriptor desc = ll::SessionDescriptor().enableDebug(true);   std::shared_ptr\u003cll::Session\u003e session = ll::Session::create(desc);   std::vector\u003cvk::MemoryPropertyFlags\u003e flagsVector = session-\u003egetSupportedMemoryFlags();   for(int i = 0; i \u003c flagsVector.size(); ++i) {  const vk::MemoryPropertyFlags \u0026flags = flagsVector[i];   std::cout \u003c\u003c \"Memory index: \" \u003c\u003c i \u003c\u003c std::endl;  std::cout \u003c\u003c \" Supported flags: \" \u003c\u003c vk::to_string(flags) \u003c\u003c std::endl;  }   return 0; }    Memory index: 0 supported flags: ['DeviceLocal'] Memory index: 1 supported flags: ['HostCoherent', 'HostVisible'] Memory index: 2 supported flags: ['HostCached', 'HostCoherent', 'HostVisible'] Memory index: 3 supported flags: ['DeviceLocal', 'HostCoherent', 'HostVisible']    The possible MemoryPropertyFlags values are:\n   Flag Description     DeviceLocal The memory is visible to the GPU.   HostVisible The memory is visible to the host (CPU).   HostCoherent If set, it indicates that read/write operations on the memory are coherent. That is, no flushing is needed for making the values visible by other consumers.   HostCached If set, it indiates that read/write operations travel through the host memory cache. Operations may be faster, but need flushing to make the values available to other consumers.   LazilyAllocated  Not used in Lluvia.    For Lluvia, the two most important memory flag tuples are:\n   Tuple Description     (DeviceLocal) The memory is visible to the GPU only. Computations will be performed on objects allocated in this memory.   (DeviceLocal, HostVisible, HostCoherent) The memory is visible to both the GPU and the host CPU. Writings to the memory from the host CPU are coherent. This memory will be used mainly for transfering data to and from the GPU.    Creation The code block below shows how to create memory objects:\nPython  C++   1 2 3 4 5 6 7 8 9 10  import lluvia as ll  session = ll.createSession(enableDebug=True)  deviceMemory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal, pageSize=32*1024*1024, exactFlagsMatch=False)  # use default page size hostMemory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal,  ll.MemoryPropertyFlagBits.HostVisible,  ll.MemoryPropertyFlagBits.HostCoherent])   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #include \"lluvia/core.h\"#include \u003cvulkan/vulkan.hpp\u003e#include \u003ciostream\u003e#include \u003cmemory\u003e int main() {   auto session = ll::Session::create(ll::SessionDescriptor().enableDebug(true));   const vk::MemoryPropertyFlags deviceFlags = vk::MemoryPropertyFlagBits::eDeviceLocal;  const vk::MemoryPropertyFlags hostFlags = vk::MemoryPropertyFlagBits::eDeviceLocal |  vk::MemoryPropertyFlagBits::eHostVisible |  vk::MemoryPropertyFlagBits::eHostCoherent;   std::shared_ptr\u003cll::Memory\u003e deviceMemory = session-\u003ecreateMemory(deviceFlags, 32*1024*1024, false);   std::shared_ptr\u003cll::Memory\u003e hostMemory = session-\u003ecreateMemory(hostFlags, 32*1024*1024, false);   return 0; }    Memories are created by passing the set of flags the memory should have. It is possible to also pass the size of a page, which defaults to 32MB, and an extra parameter to indicate if the flags should match perfectly with any of those listed by session.getSupportedMemoryPropertyFlags().\nInternally, a Memory manages regions of memory as pages. On each page, there can be several objects allocated, such as Buffer or Image.\nstateDiagram-v2 Memory --\u003e Page_0 Memory --\u003e Page_1 Memory --\u003e Page_2 state Page_0 { Buffer_0 Buffer_1 } state Page_1 { Image_1 Buffer_2 } state Page_2 { Image_2 } It is possible to query the memory attributes as:\nPython  C++   1 2 3 4  print('flags :', [p.name for p in hostMemory.memoryFlags]) print('isMappable :', hostMemory.isMappable) print('pageCount :', hostMemory.pageCount) print('pageSize :', hostMemory.pageSize)   1 2 3 4  std::cout \u003c\u003c \"flags : \" \u003c\u003c vk::to_string(hostMemory-\u003egetMemoryPropertyFlags()) \u003c\u003c std::endl; std::cout \u003c\u003c \"isMappable : \" \u003c\u003c hostMemory-\u003eisMappable() \u003c\u003c std::endl; std::cout \u003c\u003c \"pageCount : \" \u003c\u003c hostMemory-\u003egetPageCount() \u003c\u003c std::endl; std::cout \u003c\u003c \"pageSize : \" \u003c\u003c hostMemory-\u003egetPageSize() \u003c\u003c std::endl;    which prints:\nflags : ['DeviceLocal', 'HostCoherent', 'HostVisible'] isMappable : True pageCount : 0 pageSize : 33554432    In particular, the isMappable flag tells whether or not the memory space can be mapped to the host memory space. At the moment of creation, there are no actual pages allocated, and hence, pageCount equals 0.\nObject allocation There are two types of objects that can be allocated from a Memory:\ngraph Memory --\u003e Buffer Memory --\u003e Image click Buffer \"/docs/reference/objects/buffer\" \"Buffer\" click Image \"/docs/reference/image\" \"Image\" The code block below shows how to allocate a buffer and an image object. Each allocated object has an allocationInfo to see the allocation values.\nPython  C++   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import lluvia as ll  session = ll.createSession(enableDebug=True)  deviceMemory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal, pageSize=32*1024*1024)  # A 1024 byte size buffer buffer = deviceMemory.createBuffer(1024)  # A 32x32 pixels image where each pixel is of type Uint8 image = deviceMemory.createImage((32, 32), ll.ChannelType.Uint8)  print('buffer:') print(' page :', buffer.allocationInfo.page) print(' offset :', buffer.allocationInfo.offset) print(' left padding :', buffer.allocationInfo.leftPadding) print(' size :', buffer.allocationInfo.size)  print() print('image:') print(' page :', image.allocationInfo.page) print(' offset :', image.allocationInfo.offset) print(' left padding :', image.allocationInfo.leftPadding) print(' size :', image.allocationInfo.size)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #include \"lluvia/core.h\"#include \u003cvulkan/vulkan.hpp\u003e#include \u003ciostream\u003e#include \u003cmemory\u003e int main() {  auto session = ll::Session::create(ll::SessionDescriptor().enableDebug(true));   const vk::MemoryPropertyFlags deviceFlags = vk::MemoryPropertyFlagBits::eDeviceLocal;   std::shared_ptr\u003cll::Memory\u003e deviceMemory = session-\u003ecreateMemory(deviceFlags, 32*1024*1024, false);   std::shared_ptr\u003cll::Buffer\u003e buffer = deviceMemory-\u003ecreateBuffer(1024);   // 32x32 image with one uint8 color channel per pixel  ll::ImageDescriptor desc = ll::ImageDescriptor(1, 32, 32, ll::ChannelCount::C1, ll::ChannelType::Uint8);  std::shared_ptr\u003cll::Image\u003e image = deviceMemory-\u003ecreateImage(desc);   std::cout \u003c\u003c \"buffer:\" \u003c\u003c std::endl;  std::cout \u003c\u003c \" page : \" \u003c\u003c buffer-\u003egetAllocationInfo().page \u003c\u003c std::endl;  std::cout \u003c\u003c \" offset : \" \u003c\u003c buffer-\u003egetAllocationInfo().offset \u003c\u003c std::endl;  std::cout \u003c\u003c \" left padding : \" \u003c\u003c buffer-\u003egetAllocationInfo().leftPadding \u003c\u003c std::endl;  std::cout \u003c\u003c \" size : \" \u003c\u003c buffer-\u003egetAllocationInfo().size \u003c\u003c std::endl;   std::cout \u003c\u003c std::endl;  std::cout \u003c\u003c \"image:\" \u003c\u003c std::endl;  std::cout \u003c\u003c \" page : \" \u003c\u003c image-\u003egetAllocationInfo().page \u003c\u003c std::endl;  std::cout \u003c\u003c \" offset : \" \u003c\u003c image-\u003egetAllocationInfo().offset \u003c\u003c std::endl;  std::cout \u003c\u003c \" left padding : \" \u003c\u003c image-\u003egetAllocationInfo().leftPadding \u003c\u003c std::endl;  std::cout \u003c\u003c \" size : \" \u003c\u003c image-\u003egetAllocationInfo().size \u003c\u003c std::endl; }    The amount of memory reserved for a given object can be higher than the actual needed. This is because Vulkan imposes certain requirements on the allocation such as alignment.\nbuffer: page : 0 offset : 2048 left padding : 0 size : 1024 image: page : 0 offset : 0 left padding : 0 size : 2048    What’s next Check the Objects page for an overview of the objects available in Lluvia.\n","categories":"","description":"","excerpt":"Memory objects represent regions of memory that can be used to …","ref":"/docs/reference/memory/","tags":"","title":"Memory"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/gettingstarted/installation/","tags":"","title":"Installation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/gettingstarted/","tags":"","title":"Getting started"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/objects/","tags":"","title":"Objects"},{"body":"The diagram below illustrates the suggested order for reading the documentation:\ngraph Session Memory Session --\u003e Memory Memory --\u003e Objects Objects --\u003e Buffer Objects --\u003e Image Image --\u003e ImageView Session --\u003e NodeSystem NodeSystem --\u003e ComputeNode ComputeNode --\u003e ContainerNode %% Interaction click Session \"/docs/reference/session\" \"Session\" click Memory \"/docs/reference/memory\" \"Memory\" click Objects \"/docs/reference/objects\" \"Objects\" click Buffer \"/docs/reference/objects/buffer\" \"Buffer\" click Image \"/docs/reference/image\" \"Image\" click ImageView \"/docs/reference/image_view\" \"ImageView\" click NodeSystem \"/docs/reference/node_system\" \"NodeSystem\" click ComputeNode \"/docs/reference/node_system/compute_node\" \"ComputeNode\" click ContainerNode \"/docs/reference/node_system/container_node\" \"ContainerNode\" ","categories":"","description":"Reference","excerpt":"Reference","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/node_system/running/","tags":"","title":"Running"},{"body":"Dependencies   Python2. This is needed for Bazel to be able to run Python binaries\n1 2 3  choco install python2  python -m pip install jinja2     Python3 dependencies\n1  pip3 install cython numpy pytest jinja2 markupsafe     Vulkan SDK: follow the official installation instructions from LunarG.\n  Bazel: follow the official installation guide from bazel.build.\n  Clone and configure the repository Clone the Lluvia repository from Github:\n1 2  git clone https://github.com/jadarve/lluvia.git cd lluvia   Open platform/values.bzl and change the paths to Python2 and Python3 according to your instalation. Initially the file looks like this:\n1 2 3 4 5 6 7 8 9  # Linux python2_path_linux = \"/usr/bin/python2\" python3_path_linux = \"/usr/bin/python3\"  # Windows python2_path_windows = \"C:/Python27/python.exe\"  # get this value by running where.exe python3 python3_path_windows = \"C:/hostedtoolcache/windows/Python/3.7.9/x64/python3.exe\"   Build the C++ libraries 1  bazel build //lluvia/cpp/...   Run the tests to verify that your compilation runs properly:\n1  bazel test //lluvia/cpp/...   Python3 package To build the Python3 package, execute the commands below from the repository’s top-level directory.\n1 2  bazel build //lluvia/python:lluvia_wheel pip3 install bazel-bin/lluvia/python/lluvia-0.0.1-py3-none-any.whl   Open a Python3 interpreter and import lluvia package\n1 2 3  import lluvia as ll  session = ll.createSession()   If the import completes successfully, lluvia is ready to use.\n","categories":"","description":"","excerpt":"Dependencies   Python2. This is needed for Bazel to be able to run …","ref":"/docs/gettingstarted/installation/windows/","tags":"","title":"Windows 10"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/","tags":"","title":"Nodes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/examples/","tags":"","title":"Examples"},{"body":"%%{init: {'theme': 'neutral', 'themeVariables': {'fontSize': '32px', 'primaryColor': '#FF0000'}}}%% classDiagram class Session class Memory class Program class Buffer class Image class ImageView class Node class ComputeNode class ContainerNode class CommandBuffer Session \"1\" --\u003e \"*\" Memory Session \"1\" --\u003e \"*\" Node Session \"1\" --\u003e \"*\" CommandBuffer Memory \"1\" --\u003e \"*\" Buffer: allocates Memory \"1\" --\u003e \"*\" Image: allocates Image \"1\" --\u003e \"*\" ImageView: creates Node \u003c|-- ComputeNode Node \u003c|-- ContainerNode ComputeNode \"1\" --\u003e \"1\" Program ContainerNode \"1\" --\u003e \"*\" ComputeNode: contains ContainerNode \"1\" --\u003e \"*\" ContainerNode: contains ","categories":"","description":"","excerpt":"%%{init: {'theme': 'neutral', 'themeVariables': {'fontSize': '32px', …","ref":"/docs/reference/node_system/","tags":"","title":"Node system"},{"body":"Introduction Mediapipe is a cross-platform framework to create complex Computer Vision pipelines both for offline and real-time applications. It leverages popular frameworks such as OpenCV and Tensorflow to process audio, video, and run deep learning models. By integrating Lluvia into mediapipe, it is possible to speed up some of those computations by creating a GPU compute pipeline.\n  Difference 1: project scope Mediapipe is a more general framework than Lluvia. Mediapipe, at its core, is a compute graph scheduler, where each node can contain any arbitrary processing logic. The integration of third-party frameworks (e.g. OpenCV, Tensorflow, Lluvia) gives the framework its power for developing complex Computer Vision pipelines.\nLluvia, on the other hand, is specialized in creating compute pipelines running efficiently on GPU. Bringing the project to Mediapipe will enable easier integration with other frameworks and increase runtime performance of Computer Vision applications.\n On Graphs, Calculators and Packets Mediapipe uses Directed Acylic Graphs to describe the compute pipeline to be run by the framework. Each node in the graph is denoted a Calculator. Each calculator declares its inputs and outputs contract, establishing the type of packet it can handle, and defines a function to process those packets.\nGraphs are described as Protobuffers, with the configuration for each calculator. Mediapipe takes this data at runtime, instantiate each calculator, and connects it to its up and downstream neighbors according to the supplied contracts.\nPackets enter the graph through input streams and leave it through output streams. When a new packet arrives, mediapipe schedules the processing of that packet to the corresponding calculator, or enqueues it if it is busy.\nThe figure below illustrates a mediapipe graph for performing edge detection on the GPU. Each calculator receives GPU image packets and schedules execution on the available device.\nDifference 2: packets and graph scheduling A packet in Mediapipe is an independent piece of data that travels through the calculator graphs. This enables Mediapipe to schedule running several calculators concurrently, thus potentially increasing performance.\nIn Lluvia, nodes connected through inputs and outputs do not allocate new memory on each run of the node. Instead, all the memory is allocated at node initialization time, and exposed through the node’s ports. Then, the whole graph is scheduled to run on the GPU device in one go. This reduces the delay in computations as avoids cross-talk between the host CPU and the GPU to synchronize individual node execution.\n Lluvia as a mediapipe dependency Mediapipe, as well as Lluvia, are built using Bazel. As a consequence, the integration of Lluvia can be done by including the project as a Bazel dependency into Mediapipe repository. The current approach to achieve this is through the use of an auxiliary repository, lluvia-mediapipe, that contains the LluviaCalculator node to run GPU compute-pipelines as a Mediapipe calculator. The build instructions are available in the mediapipe integration guide. The process is as follows:\n Clone Mediapipe repository alongside Lluvia. Configure Mediapipe’s Bazel workspace to build in your host machine. Include Lluvia as a dependency to Mediapipe. Clone lluvia-mediapipe repository inside Mediapipe to enable building its targets. Run the tests included in the repository to validate the build.  The directory structure of the three projects should look like this:\nlluvia \u003c-- lluvia repository mediapipe \u003c-- mediapipe repository ├── BUILD.bazel ├── LICENSE ├── ... ├── mediapipe \u003c-- │ ├── BUILD │ ├── calculators │ ├── examples │ ├── framework │ ├── gpu │ ├── ... │ ├── lluvia-mediapipe \u003c-- lluvia-mediapipe repository ├── ... ├── .bazelrc └── WORKSPACE Once Mediapipe builds correctly, it is possible to create graphs that include the LluviaCalculator.\nThe LluviaCalculator The LluviaCalculator is in charge of initializing Lluvia, binding input and output streams from mediapipe to lluvia ports, and running a given compute pipeline. The figure below illustrates a basic mediapipe graph utilizing lluvia, while the code below shows the graph description using Protobuffer text syntax:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  input_stream: \"input_stream\"output_stream: \"output_stream\"node: { calculator: \"LluviaCalculator\" input_stream: \"IN_0:input_stream\" output_stream: \"OUT_0:output_stream\" node_options { [type.googleapis.com/lluvia.LluviaCalculatorOptions]: { enable_debug: true library_path: \"path to .zip node library file\" script_path: \"path to .lua script defining the main container node\" container_node: \"mediapipe/examples/Passthrough\" input_port_binding: { mediapipe_tag: \"IN_0\" lluvia_port: \"in_image\" } } }}  where:\n The enable_debug flag tells whether or not the Vulkan debug extensions used by Lluvia should be loaded during session creation. This flag might be set to false in production applications to improve runtime performance. The library_path declare paths to node libraries (a .zip file) containing Lluvia nodes (Container and Compute). This attribute can be repeated several times. The script_path is the path to a lua script declaring a ContainerNode that Lluvia will instantiate as the “main” node to run inside the calculator. input_port_binding, maps mediapipe input tags to the main ContainerNode port. In the example above, mediapipe’s input tag IN_0 is mapped to lluvia’s in_image port.  Examples lluvia-mediapipe includes two applications, single_image and webcam to run on the host system. The single_image app, as the name suggests, reads the content of a single image and feeds it to a Mediapipe graph.\nThe command below executes the binary with a graph configured to run the lluvia/color/BGRA2Gray compute node to convert from the BGRA input to gray scale:\n1 2 3 4 5  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/single_image:single_image -- \\  --input_image=${HOME}/git/lluvia/lluvia/resources/mouse.jpg \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/graph.pbtxt   where ${HOME}/git is the base folder where Lluvia and Mediapipe are cloned. Change this according to your setup.\nA more sophisticated example is running the Horn and Schunck optical flow algorithm inside of Mediapipe. The webcam binary opens the default capture device using OpenCV and transfers the captured frames the compute graph. The graph is a single LluviaCalculator running several nodes:\n1 2 3 4  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/webcam:webcam -- \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/graph.pbtxt   where --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/graph.pbtxt is the path to Mediapipe’s graph to be run by the app, and --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/script.lua points to a Lua script defining the Container node to run inside of the LluviaCalculator.\n@startuml skinparam linetype ortho state LluviaCalculator as \"LluviaCalculator\" { state input_stream as \"IN_0:input_stream\" \u003c\u003cinputPin\u003e\u003e state output_stream as \"OUT_0:output_stream\" \u003c\u003coutputPin\u003e\u003e state ContainerNode as \"mediapipe/examples/HornSchunck\" { state in_image \u003c\u003cinputPin\u003e\u003e state BGRA2Gray state HS as \"HornSchunck\" state Flow2RGBA state RGBA2BGRA input_stream -down-\u003e in_image in_image -down-\u003e BGRA2Gray BGRA2Gray -down-\u003e HS: in_gray HS -down-\u003e Flow2RGBA: in_flow Flow2RGBA -down-\u003e RGBA2BGRA: in_rgba RGBA2BGRA -down-\u003e out_image \u003c\u003coutputPin\u003e\u003e } out_image -down-\u003e output_stream \u003c\u003coutputPin\u003e\u003e } @enduml First, the input image is transformed from BGRA color space to gray scale. Next, the images are fed to the HornSchunck container node to compute optical flow. The estimated flow is then converted to color using the Flow2RGBA compute node, and finally, the RGBA output is converted to BGRA to proper rendering in the window opened by OpenCV.\nDifference 3: calculators as code vs. nodes as data In Mediapipe, every Calculator must be compiled and integrated into the binary at build time, thus requiring rebuilding every time a new Calculator must be added or modified.\nLluvia describes nodes as a pair of Lua and GLSL (for ComputeNode) files that are compiled and packaged into a node library as a .zip file. Once packaged, the library can be imported on any runtime where Lluvia runs. This eases the developer experience as one can develop nodes in a higher-level environment, using Python in a Jupyter notebook for instance, package the nodes in a node library and then use them in any environment (Mediapipe for instance).\n Discussion This article presented the integration of Lluvia into the Mediapipe project. By added the project into Mediapipe, it is possible to leverage the GPU compute-pipeline capabilities of Lluvia to speed up parts of complex Computer Vision applications.\nThe integrations between thw two projects is achieved through the LluviaCalculator which runs any arbitrary ContainerNode. This calculator is in early stages of development, and feedback is very welcomed. Some immediate improvements include:\n Support GPUImageFrame input and output packets. Currently, the calculator only accepts CPU ImageFrame packets, thus introducing some latency while copying data from CPU memory space to the GPU. Support Mediapipe side packets to send configuration updates to the calculator. Include more configuration attributes (e.g. node parameters) in the Protobuffer type.  And finally, testing the integration in other platforms such as Android.\nReferences  Mediapipe lluvia-mediapipe Bazel  ","categories":"","description":"Integration of Lluvia into Mediapipe to create complex Computer Vision pipelines.","excerpt":"Integration of Lluvia into Mediapipe to create complex Computer Vision …","ref":"/blog/2022/10/08/mediapipe-integration/","tags":"","title":"Mediapipe integration"},{"body":" Jupyter notebook: A Jupyter notebook with the code in this article is available in Google Colab. Check it out!  Background Camera undistort is the process by which distortions generated by the optics used in the camera during the capture process are corrected in software. The process requires a mathematical model of the distortion, and a calibration procedure to estimate the parameters of such model given actual images.\nAn overview of the camera modeling is pressented in the Computer Vision book of Szeliski and the Multiple View Geometry book of Hartley and Zisserman, as well as the articles of Zhang, Wei and Ma.\nThere are several calibration toolboxes available for estimating the camera model from a series of images:\n OpenCV calibration routines. Matlab camera calibration App.  Any of such frameworks can be used to estimate the camera model parameters. Those parameters are the input to the undistort method presented in this article to rectify raw captured images.\nCamera model The figure below illustrates the camera model.\nThe 3D point $\\mathbf{x} \\in \\mathbb{R}^3$ is expressed relative to the camera body fixed frame. It projects onto the camera image plane as pixel $\\mathbf{p} := (u, v)^\\top \\in \\mathbb{R}^2$ as\n$$ \\begin{equation} \\begin{pmatrix} \\mathbf{p} \\\\ 1 \\end{pmatrix} := \\begin{pmatrix} u \\\\ v \\\\ 1 \\end{pmatrix} = \\frac{\\mathbf{K} \\mathbf{x}}{ \\left\u003c e_3, \\mathbf{x} \\right\u003e} \\end{equation} $$\nwhere $\\mathbf{K} \\in \\mathbb{R}^{3\\times3}$ is the camera intrinsics matrix, $e_3 := (0, 0, 1)^\\top$, and $\\left\u003c e_3, \\mathbf{x} \\right\u003e$ is the dot product between the two vectors. The units of $\\mathbf{p}$ are actual pixel coordinates in the ranges $u \\in [0, W)$ and $v \\in [0, H)$, with $W$ and $H$ denoting the image width and height respectively.\nGiven a pixel point, the corresponding 3D coordinate $\\bar{\\mathbf{x}}$ in the image plane is defined as:\n$$ \\begin{equation} \\bar{\\mathbf{x}} := \\begin{pmatrix} \\bar{x} \\\\ \\bar{y} \\\\ \\bar{z} \\\\ \\end{pmatrix} = \\mathbf{K}^{-1} \\begin{pmatrix} \\mathbf{p} \\\\ 1 \\end{pmatrix} \\end{equation} $$\nNotice that this projection does return $\\bar{\\mathbf{x}}$ and not the original 3D point $\\mathbf{x}$. To return the actual 3D position in the world, the depth information is needed to project $\\bar{\\mathbf{x}}$ outside of the image plane to the world.  Standard distortion model The standard distortion model is formed by two components:\n A radial component parameterized by three coefficients: $k_1$, $k_2$, and $k_3$. A tangential component with two parameters: $p_1$ and $p_2$.  The radial distortion component for a given pixel $\\mathbf{p}$ is computed as\n$$ \\begin{equation} \\bar{\\mathbf{x}}_r := R \\begin{pmatrix} \\bar{x} \\\\ \\bar{y} \\\\ 0 \\end{pmatrix} \\end{equation} $$\nwhere $R \\in \\mathbb{R}$ is\n$$ \\begin{equation} R = k_1 r^2 + k_2 r^4 + k_3 r^6 \\end{equation} $$\nwith\n$$ \\begin{equation} r^2 = \\bar{x}^2 + \\bar{y}^2 \\end{equation} $$\nand $\\bar{x}, \\bar{y}$ are the $x$ and $y$ coordinates of the projection of pixel $\\mathbf{p}$ using equation (2).\nThe tangential distortion is computed as:\n$$ \\begin{equation} \\bar{\\mathbf{x}}_p := \\begin{pmatrix} 2 p_1 \\bar{x}\\bar{y} + p_2(r^2 + 2\\bar{x}^2) \\\\ p_1(r^2 + 2 \\bar{y}^2) + 2 p_2 \\bar{x}\\bar{y} \\\\ 0 \\end{pmatrix} \\end{equation} $$\nFinally, the undistorted image plane coordinates $\\bar{\\mathbf{x}}_u$ is computed as:\n$$ \\begin{equation} \\bar{\\mathbf{x}}_u = \\bar{\\mathbf{x}} + \\bar{\\mathbf{x}}_r + \\bar{\\mathbf{x}}_p \\end{equation} $$\nGiven $\\bar{\\mathbf{x}}_u$, the corresponding undistorted pixel coordinate is:\n$$ \\begin{equation} \\begin{pmatrix} \\mathbf{p}_u \\\\ 1 \\end{pmatrix} := \\begin{pmatrix} u_u \\\\ v_u \\\\ 1 \\end{pmatrix} = \\frac{\\mathbf{K} \\bar{\\mathbf{x}}_u}{ \\left\u003c e_3, \\bar{\\mathbf{x}}_u \\right\u003e} \\end{equation} $$\nThe convention for the tangential parameters $p_1$ and $p_2$ is the same to that of OpenCV. However, this convention is flipped to respect to that presented in the article of Wei and Ma.  The figures below illustrate the effects of the radial and tangential distortion. A possitive value of $k_1$ creates a barrel effect, while a negative value generates a pincushion effect. For the tangential parameters, $p_1$ models missalignment between the image sensor and the image plane in the $y$ axis, while $p_2$ models such missalignment in the $x$ axis.\n        Implementation The camera undistort procedure is implemented as a single ComputeNode with the following interface:\n@startuml skinparam linetype ortho state CameraUndistort as \"CameraUndistort_rgba8ui\" { state in_rgba \u003c\u003cinputPin\u003e\u003e state in_camera \u003c\u003cinputPin\u003e\u003e state out_rgba \u003c\u003coutputPin\u003e\u003e } note top of CameraUndistort Parameters ---------- camera_model : int. Defaults to 0. The camera model used for rectifying the image. Possible values are: * 0: standard model end note @enduml The node explicitly requires rgba8ui images to be bound to the node. The output out_rgba is allocated by the node. The in_camera is a UniformBuffer storing the camera model. This model is defined by the ll_camera struct in GLSL as:\nGLSL   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  struct ll_camera {   // The camera intrinsic matrix. Used to project 3D points expressed in the camera coordinate frame  // to the image plane and convert to pixel coordinates.  mat3 K;   // The inverse camera intrinsic matrix. Used to convert from pixel to image plane coordinates.  mat3 Kinv;   // Radial distortion coefficients. For the standard camera model,  // only the first 3 coefficients are used (XYZ).  vec4 radialDistortion;   // Tangential distortion coefficients. Only the first 2 coefficients are used (XY).  vec4 tangentialDistortion; };    Uniform buffers are a special type of buffers used to store small data structures used in graphics and compute pipelines. The Vulkan tutorial on Uniform Buffers is a good read on how they are used in general. Notice that the ll_camera uses GLSL types such as mat3 and vec4. In the host CPU, one must use corresponding types and follow the byte alignmnet rules to make the buffer usable in the GPU. The alignment rules are defined by the STD140 layout rules. For the ll_camera struct, the mat3 attributes must be transferred as a matrix of 4 rows and 3 columns in order to meet the alignment requirements.\nMatrix storage in GLSL In GLSL, matrices are stored in column-major order. For a given matrix M indexed as M[i, j] where i and j are the row and column indexes, respectively, the elements M[i, j] and M[i, j+1] are stored contiguously in memory. This is different, for instance, to numpy’s default ordering as row-major.  The code block below shows a complete example on how to run the lluvia/camera/CameraUndistort_rgba8ui node using a dummy camera model with radial and tangential distortion:\nPython   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  import lluvia as ll import lluvia.util as ll_util import numpy as np import matplotlib.pyplot as plt  session = ll.createSession()  # memory to store the input and output images memory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal)  # memory to store the uniform buffer with the camera parameters host_memory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal,  ll.MemoryPropertyFlagBits.HostVisible,  ll.MemoryPropertyFlagBits.HostCoherent])  # read a sample image sampleImage = ll_util.readSampleImage('koala')  # draw a grid on top of the sample image Yrange = np.arange(0, sampleImage.shape[0], 128) Ylines = np.concatenate([n + Yrange for n in range(4)])  Xrange = np.arange(0, sampleImage.shape[1], 128) Xlines = np.concatenate([n + Xrange for n in range(4)])  sampleImage[Ylines, ...] = 0 sampleImage[:, Xlines, ...] = 0  # the input image view must be sampled. This example uses nearest neighbor interpolation in_rgba = memory.createImageViewFromHost(sampleImage,  filterMode=ll.ImageFilterMode.Nearest,  addressMode=ll.ImageAddressMode.Repeat,  normalizedCoordinates=False,  sampled=True)  ################################################### # Camera parameters W = float(in_rgba.width) H = float(in_rgba.height)  # Dummy camera matrix K = np.array([[W, 0, 0.5*(W -1)],  [0, H, 0.5*(H -1)],  [0, 0, 1] ], dtype=np.float32, order='F') Kinv = np.linalg.inv(K) radialDistortion = np.array([0.5, 0, 0, 0,], dtype=np.float32) tangentialDistortion = np.array([0.1, 0, 0, 0], dtype=np.float32)  # align the matrices according to the STD140 rules (column major, 4-component vectors) K_aligned = np.zeros((4,3), dtype=np.float32, order='F'); K_aligned[:3, :3] = K Kinv_aligned = np.zeros((4,3), dtype=np.float32, order='F'); Kinv_aligned[:3, :3] = Kinv  # create bytes buffer from matrices buf = K_aligned.tobytes(order='F') + Kinv_aligned.tobytes(order='F') + radialDistortion.tobytes() + tangentialDistortion.tobytes() npBuf = np.frombuffer(buf, dtype=np.uint8)  # in_camera uniform buffer in_camera = host_memory.createBufferFromHost(npBuf, usageFlags=[ll.BufferUsageFlagBits.TransferSrc,  ll.BufferUsageFlagBits.TransferDst,  ll.BufferUsageFlagBits.UniformBuffer])  ################################################### # Compute node CameraUndistort = session.createComputeNode('lluvia/camera/CameraUndistort_rgba8ui') CameraUndistort.setParameter('camera_model', ll.Parameter(1)) # standard model CameraUndistort.bind('in_rgba', in_rgba) CameraUndistort.bind('in_camera', in_camera) CameraUndistort.init()  CameraUndistort.run()  out_rgba = CameraUndistort.getPort('out_rgba')  ################################################### # Plotting fig = plt.figure(figsize=(15, 8)); fig.set_tight_layout(True) plt.subplot2grid((1,2), (0,0)); plt.imshow(in_rgba.toHost()[..., :3]); plt.title('in_rgba') plt.subplot2grid((1,2), (0,1)); plt.imshow(out_rgba.toHost()[..., :3]); plt.title('out_rgba') plt.show()    Lines 36 to 60 create the uniform buffer containing the camera model. Lines 42 and 45 create the camera intrinsics matrix K and its inverse Kinv. Then, in lines 50-51, those matrices are aligned to meet the std140 requirements; in this case, storing each matrix in a 4x3 matrix in column-major ordering (using order='F' in numpy). Finally, lines 54-55 concatenates all camera parameters to create a single numpy array npBuf which is then used to create the in_camera uniform buffer in lluvia.\n  Runtime performance A Razer Blade laptop running Ubuntu 22.04LTS was used for the runtime analysis. The laptop is equipped with an Intel i7-11800H processor, and the following Vulkan devices as reported by the code block below:\nPython   1 2 3  import lluvia as ll for dev in ll.getAvailableDevices():  print(dev)     NVIDIA GeForce RTX 3070 Laptop GPU. Intel(R) UHD Graphics (TGL GT1). llvmpipe (LLVM 13.0.1, 256 bits). This is a CPU implementation of the Vulkan API shipped with the Mesa drivers.  In addition, the cv2.undistort() function from OpenCV is considered for reference. Five resolutions are used in the evaluation: VGA 640x480, HD 1280x720, FHD 1920x1080, WQHD 2560x1440, and UHD 3840x2160. For each resolution, the algorithm is run for 1000 iterations and the median runtime is extracted. The figure and table belows show the runtime for each device and resolution combination.\n     Resolution Device name Runtime median ms     VGA 640x480 Intel UHD Graphics 0.00235    RTX 3070 0.013888    llvmpipe 0.604263    OpenCV 2.04252   HD 1280x720 Intel UHD Graphics 0.007734    RTX 3070 0.03728    llvmpipe 1.50221    OpenCV 6.38165   FHD 1920x1080 Intel UHD Graphics 0.0151045    RTX 3070 0.07456    llvmpipe 3.17916    OpenCV 17.1453   WQHD 2560x1440 Intel UHD Graphics 0.0262145    RTX 3070 0.109344    llvmpipe 5.97528    OpenCV 22.8469   UHD 3840x2160 Intel UHD Graphics 0.058583    RTX 3070 0.242688    llvmpipe 17.6178    OpenCV 49.477    Integrated vs Discrete GPU performance Notice that the Intel UHD Graphics device reports lower runtime than the discrete Nvidia RTX 3070 GPU. It is not clear why this is the case, as the Nvidia GPU has more compute resources than the Intel integrated graphics.  Also, notice how the llvmpipe CPU device is between three to four times faster than the OpenCV function. However, both CPU devices are 2 orders of magnitude slower than the Nvidia and Intel GPU devices.\nDiscussion This post showed how to run the camera undistort node in Lluvia. The node takes as input an RGBA image and a camera model stored in a uniform buffer in the GPU, and produces an RGBA output image. The camera model stored in the uniform model must follow the GLSL std140 layout rules. In terms of runtime performance, the GPU implementation is several orders of magnitude faster than the OpenCV default implementation.\nFuture pieces of work includes:\n Expose the interpolation coordinates for undistorting the images as a new compute node. These coordinates could be cached in order to save computations on every node invocation. Clip the undistorted image to a given area according to the camera model. This will be useful to avoid wasted pixels in the output, as shown in the examples. Support for more image formats, such as r8ui and floating point channel types.  References  OpenCV camera calibration routines. Matlab calibration app. Vulkan tutorial on Uniform Buffers. GLSL STD140 memory layout. Mesa llvmpipe. Zhang, Z., 2000. A flexible new technique for camera calibration. IEEE Transactions on pattern analysis and machine intelligence, 22(11), pp.1330-1334. Microsoft Technical Report. Wei, G.Q. and De Ma, S., 1994. Implicit and explicit camera calibration: Theory and experiments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5), pp.469-480. DOI. Szeliski, R., 2010. Computer vision: algorithms and applications. Springer Science \u0026 Business Media. Book. Hartley, R. and Zisserman, A., 2003. Multiple view geometry in computer vision. Cambridge university press. Book  ","categories":"","description":"Presents new nodes for undistorting images given a camera calibration model with radial and tangential distortion.","excerpt":"Presents new nodes for undistorting images given a camera calibration …","ref":"/blog/2022/09/18/camera-undistort/","tags":"","title":"Camera undistort"},{"body":" Jupyter notebook: A Jupyter notebook with the code in this article is available in Google Colab. Check it out!  Background The Horn and Schunck variational method for computing optical flow is one of the seminal works in the field. It introduces the idea of using a global smoothness constrain on the estimated optical flow. This constrain helps the numerical solution to find a good flow estimate even in image regions with poor texture.\nLet $\\mathbf{E}(x, y, t)$ be the image brightness at point $(x, y)$ and time $t$. Considering the constant brightness assumption, where the change in brightness is zero, that is,\n$$ \\frac{d \\mathbf{E}}{d t} = 0 $$\nTaking the partial derivatives over $(x, y, t)$, one has:\n$$ \\frac{\\partial \\mathbf{E}}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial \\mathbf{E}}{\\partial y} \\frac{\\partial y}{\\partial t} + \\frac{\\partial \\mathbf{E}}{\\partial t} = 0 $$\nFor convenience, let:\n$$ \\begin{align*} \\mathbf{E}_x \u0026= \\frac{\\partial \\mathbf{E}}{\\partial x} \\\\ \\mathbf{E}_y \u0026= \\frac{\\partial \\mathbf{E}}{\\partial y} \\\\ \\mathbf{E}_t \u0026= \\frac{\\partial \\mathbf{E}}{\\partial t} \\end{align*} $$\nbe the image gradient in the $x$ and $y$ directions, and the partial derivative in time, respectively, and\n$$ \\begin{align} u \u0026= \\frac{\\partial x}{\\partial t} \\\\ v \u0026= \\frac{\\partial y}{\\partial t} \\end{align} $$\nbe the $x$ and $y$ components of the optical flow, respectively. The constant brightness equation is then\n$$ \\mathbf{E}_x u + \\mathbf{E}_y v + \\mathbf{E}_t = 0 $$\nwhich is the basis for the differential methods for computing optical flow (e.g. Lukas-Kanade).\nMinimization Differential methods for estimating optical flow try to minimize the cost function\n$$ \\epsilon_b = \\mathbf{E}_x u + \\mathbf{E}_y v + \\mathbf{E}_t $$\nthat is, to try to find values $(u, v)$ of the optical flow such that the constant brightness constrain is maintained. Notice that there is a single cost funcion and two unknowns $(u, v)$. To solve this, the Horn and Schunck algorithm adds a smoothness constrain based on the average value of the flow in a neighborhood, as\n$$ \\epsilon_c^2 = (\\bar{u} - u )^2 + (\\bar{v} - v)^2 $$\nCombining both cost functions, one has\n$$ \\epsilon^2 = \\alpha^2 \\epsilon_b^2 + \\epsilon_c^2 $$\nFrom these equations, a numerical solution is derived. The reader is encouraged to go to the paper for more details. The iterative solution for $(u, v)$ is\n$$ \\begin{align*} u^{n+1} \u0026= \\bar{u}^n - \\mathbf{E}_x \\frac{\\mathbf{E}_x \\bar{u}^n + \\mathbf{E}_y \\bar{v}^n + \\mathbf{E}_t}{\\alpha^2 + \\mathbf{E}_x^2 + \\mathbf{E}_y^2} \\\\ v^{n+1} \u0026= \\bar{v}^n - \\mathbf{E}_y \\frac{\\mathbf{E}_x \\bar{u}^n + \\mathbf{E}_y \\bar{v}^n + \\mathbf{E}_t}{\\alpha^2 + \\mathbf{E}_x^2 + \\mathbf{E}_y^2} \\end{align*} $$\nwhere $(u^{n+1}, v^{n+1})$ is the estimated optical flow at iteration $n + 1$, using the estimated flow at previous iterations and image parameters computed from an image pair.\nImplementation The figure below illustrates the pipeline implementing the algorithm:\n@startuml skinparam linetype ortho state HS as \"HornSchunck\" { state in_gray \u003c\u003cinputPin\u003e\u003e state ImageProcessor state ImageNormalize_uint_C1 state NI_1 as \"NumericIteration 1\" state NI_2 as \"NumericIteration 2\" state NI_3 as \"NumericIteration 3\" state NI_N as \"NumericIteration N\" in_gray -down-\u003e ImageProcessor in_gray -down-\u003e ImageNormalize_uint_C1 ImageNormalize_uint_C1 -down-\u003e ImageProcessor: in_gray_old ImageProcessor -down-\u003e NI_1: in_image_params ImageProcessor -down-\u003e NI_2 ImageProcessor -down-\u003e NI_3 ImageProcessor -down-\u003e NI_N: in_image_params NI_1 -\u003e NI_2 NI_2 -\u003e NI_3 NI_3 -\u003e NI_N: ... NI_N -\u003e NI_1: in_flow, used for next image iteration NI_N -down-\u003e out_flow \u003c\u003coutputPin\u003e\u003e ImageNormalize_uint_C1 -down\u003e out_gray \u003c\u003coutputPin\u003e\u003e } note top of HS Parameters ---------- alpha : float. Defaults to 0.05. Regularization gain. iterations : int. Defaults to 1. Number of iterations run to compute the optical flow. float_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray and out_flow will be of this floating point precision. end note @enduml The HornSchunck is a ContainerNode that instantiates several ComputeNode implementing the algorithm. In particular, the ImageProcessor node computes image parameters from the pair of images in_gray and in_gray_old. Those parameters are transfered to the instances of NumericIteration through in_image_params, organized as follows:\n in_image_params.x: X component of the image gradient in_image_params.y: Y component of the image gradient in_image_params.z: temporal derivative between in_gray and in_gray_old. in_image_params.w: gain for this pixel computed from image gradient and alpha parameter.  This packaging of the image parameters is convenient as all values are packed together in a singe RGBA pixel. The floating point precision of this, and the estimated optical flow is controlled by the float_precision parameter.\nThe NumericIteration node takes the image parameters and a prior estimation of the optical flow, in_flow, and computes the next iteration of the flow field. The algorithm requires several iterations for the estimated flow to be of acceptable quality. In the figure above, the last iteration is denoted as NumericIteration_N and it feeds its output back as input to the first one, as well as the output of the HornSchunck node. The number of iterations is controlled by the iterations parameter.\nThe code block below shows how to run a simple pipeline:\n@startuml skinparam linetype ortho state RGBA2Gray state HS as \"HornSchunck\" state Flow2RGBA RGBA2Gray -down-\u003e HS: in_gray HS -down-\u003e Flow2RGBA: in_flow @enduml where RGBA2Gray converts an input RGBA image to gray scale, HornSchunck computes the optical flow, and Flow2RGBA converts the optical flow to color representation.\nPython   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  import lluvia as ll import lluvia.util as ll_util import matplotlib.pyplot as plt  # read two images as numpy arrays frame_0 = ll_util.readRGBA('path to first image...') frame_1 = ll_util.readRGBA('path to second image...')  # global session and memory objects session = ll.createSession() memory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal)  # this is the input of the comple pipeline in_rgba = memory.createImageViewFromHost(frame_0)  RGBA2Gray = session.createComputeNode('lluvia/color/RGBA2Gray') RGBA2Gray.bind('in_rgba', in_rgba) RGBA2Gray.init() RGBA2Gray.run() # run the node immediately in order to populate out_gray with valid values  in_gray = RGBA2Gray.getPort('out_gray')  HornSchunck = session.createContainerNode('lluvia/opticalflow/HornSchunck/HornSchunck') HornSchunck.setParameter('alpha', ll.Parameter(0.05)) HornSchunck.setParameter('iterations', ll.Parameter(1000)) HornSchunck.setParameter('float_precision', ll.Parameter(ll.FloatPrecision.FP32.value)) HornSchunck.bind('in_gray', in_gray)  # when the node is initialized, it transfers the content of in_gray to out_gray. HornSchunck.init()  out_gray = HornSchunck.getPort('out_gray') out_flow = HornSchunck.getPort('out_flow')  # Convert the optical flow field to color images flow2RGBA = session.createComputeNode('lluvia/viz/Flow2RGBA') flow2RGBA.setParameter('max_flow', ll.Parameter(float(2))) flow2RGBA.bind('in_flow', out_flow) flow2RGBA.init()  out_flow_rgba = flow2RGBA.getPort('out_rgba')  duration = session.createDuration()  # Record the command buffer to run the pipeline in one go cmdBuffer = session.createCommandBuffer() cmdBuffer.begin() cmdBuffer.run(RGBA2Gray) cmdBuffer.memoryBarrier() cmdBuffer.durationStart(duration) # start recording the duration to measure runtime cmdBuffer.run(HornSchunck) cmdBuffer.memoryBarrier() cmdBuffer.durationEnd(duration) # stop recording duration cmdBuffer.run(flow2RGBA) cmdBuffer.end()  # copy the content of the second frame to the in_rgba image before running the whole pipeline in_rgba.fromHost(frame_1)  # run the pipeline session.run(cmdBuffer)  # print runtime in milliseconds print('{0:.02f}ms'.format(duration.nanoseconds / 1e6))  fig = plt.figure(figsize=(10, 6)); fig.set_tight_layout(True) plt.subplot2grid((1,2), (0, 0)); plt.imshow(out_gray.toHost(), vmin=0, vmax=1, cmap='gray') plt.subplot2grid((1,2), (0, 1)); plt.imshow(out_flow_rgba.toHost()) plt.show()    Evaluation on the Middlebury dataset The Middlebury optical flow dataset from Baker et. al. provides several real-life and synthetic image sequences with ground truth optical flow. The figures below shows the estimated optical flow for the test sequences whose ground truth is available.\nThe Horn ans Schunck algorithm is not well suited for large pixel displacements. Considering this, the input images are scaled to half before entering the compute pipeline. The ground truth flow is scaled accordingly in order to be compared with the estimated flow. The Endpoint Error measures the different in magnitude between the ground truth and the estimation, it is computed as:\n$$ EE = \\sqrt{(u - u_\\text{gt})^2 + (v - v_\\text{gt})^2} $$\nThe algorithm is configured as follows:\n alpha: 15.0/255 iterations: 2000 float_precision: FP32  In general, the estimated optical flow yields acceptable results in image regions with small displacements (e.g. Dimetrodon, Grove2, Hydrangea, and RubberWhale). In image regions with large displacements, the method is not able to compute a good results, as can be visualized in the Urban2 and Urban3 sequences.\nThe results reported in this post were run on a Razer Blade 2021 Laptop equipped with an Nvidia RTX 3070 GPU. The runtime is reported in the title of each figure, and is in the order of 20 milliseconds for most of the image sequences. Section runtime performance evaluates the performance of the algorithm on different devices, resolutions, and floating point precisions.\n                \nRuntime performance For the runtime analysis of the algorithm, two GPU devices were used:\n A Nvidia GTX 1080 Desktop GPU. A Nvidia RTX 3070 Laptop GPU running on a Razer Blade 2021.  The Horn and Schunck pipeline is configured using the same number of iterations used for the Middlebury evalatuon, that is, iterations = 2000. The pipeline is configured for 5 different image resolutions (VGA 640x480, HD 1280x720, HD 1920x1080, WQHD 2560x1440, UHD 3840x2160). For each resolution, the pipeline is run both using FP16 and FP32 floating point precision. The table and figure below show the runtime performance for each configuration.\n   Resolution Float precision Device Runtime median (ms)     VGA 640x480 FP16 GTX 1080 68.8196     RTX 3070 39.4354    FP32 GTX 1080 97.5005     RTX 3070 63.6458   HD 1280x720 FP16 GTX 1080 193.977     RTX 3070 115.626    FP32 GTX 1080 279.538     RTX 3070 175.635   HD 1920x1080 FP16 GTX 1080 429.256     RTX 3070 257.624    FP32 GTX 1080 623.555     RTX 3070 386.718   WQHD 2560x1440 FP16 GTX 1080 757.101     RTX 3070 449.536    FP32 GTX 1080 1099.35     RTX 3070 682.558   UHD 3840x2160 FP16 GTX 1080 1694.16     RTX 3070 1010.16    FP32 GTX 1080 2453.45     RTX 3070 1551.34    It is not surprising that the RTX 3070 GPU is faster than the GTX 1080, as the former is of a newer generation than the latter.\n  Discussion This post presented a GPU implementation of the Horn and Schunck optical flow algorithm. Evaluation in the Middlebury test sequences show the validity of the implementation. A runtime performance analysis was conducted on two GPUs using several image resolutions and floatin point precisions.\nFuture work includes:\n Implementing a pyramidal scheme, for instance that of Llopis et. al., to improve the accuracy of the algorithm in presence of large displacements. Use the smoothness constrain and numerical scheme in the FlowFilter algorithm to improve the accuracy.  References  Horn, Berthold KP, and Brian G. Schunck. “Determining optical flow.” Artificial intelligence 17.1-3 (1981): 185-203. Google Scholar. Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J. and Szeliski, R., 2011. A database and evaluation methodology for optical flow. International journal of computer vision, 92(1), pp.1-31. Google Scholar. Meinhardt-Llopis, E. and Sánchez, J., 2013. Horn-schunck optical flow with a multi-scale strategy. Image Processing on line. Google Scholar Adarve, Juan David, and Robert Mahony. “A filter formulation for computing real time optical flow.” IEEE Robotics and Automation Letters 1.2 (2016): 1192-1199. Google Scholar  ","categories":"","description":"GPU implementation of Horn and Schunck's variational method for estimating optical flow.","excerpt":"GPU implementation of Horn and Schunck's variational method for …","ref":"/blog/2022/08/07/implementing-the-horn-and-schunck-optical-flow-algorithm/","tags":"","title":"Implementing the Horn and Schunck optical flow algorithm"},{"body":" Jupyter notebook: A Jupyter notebook with the code in this article is available in Google Colab. Check it out!  GPU devices support several floating point number precisions, where precision refers to the number of bits used for representing a given number. Typical representations are:\n FP16: or half precision. Numbers are represented in 16 bits. FP32: or single precision. It uses 32 bits for representing a number. FP64: or doble precision. 64 bits are used for represeting a number.  FP64 is used when numerical precision is required, while FP16 is suitable for fast, less exact calculations, and FP32 sits in the middle. The IEEE 754 standard defines the specification of floating point numbers used in modern computers. It defines the rules for interpreting the bit fields that form a number, as well as the arithmetic rules to process them.\nThe Vulkan API offers support for the three floating point precisions. However, not all GPUs support every format. The Vulkan GPU Info page is great tool to check support for a given feature.\nImprovements in runtime performance Smaller bit representation of floating point numbers have an advantage in terms of runtime performance. Consider the case of a RGBA image. If the image channel type is ll.ChannelType.Float16, the four pixel values will fit in 8 bytes, compared to the 16 bytes needed if ll.ChannelType.Float32 was used. This reduction in memory footprint increases the pixel transfer rate from memory to the compute device.\nTo illustrate this, let’s consider the optical flow filter node. The code below configures the flowfilter algorithm both with ll.FloatPrecision.FP16 and ll.FloatPrecision.FP32, it runs each node for N = 10000 iterations and collects its runtime using the duration probe.\nPython   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  import lluvia as ll import numpy as np  session = ll.createSession() memory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal])  host_rgba = np.zeros((1016, 544, 4), dtype=np.uint8) in_rgba = memory.createImageViewFromHost(host_rgba)  RGBA2Gray = session.createComputeNode('lluvia/color/RGBA2Gray') RGBA2Gray.bind('in_rgba', in_rgba) RGBA2Gray.init()  N = 10000 runtimeMilliseconds = {  ll.FloatPrecision.FP16 : np.zeros((N), dtype=np.float32),  ll.FloatPrecision.FP32 : np.zeros((N), dtype=np.float32) }  for precision in [ll.FloatPrecision.FP32, ll.FloatPrecision.FP16]:   flowFilter = session.createContainerNode('lluvia/opticalflow/flowfilter/FlowFilter')  flowFilter.setParameter('levels', ll.Parameter(2))  flowFilter.setParameter('max_flow', ll.Parameter(2))  flowFilter.setParameter('smooth_iterations', ll.Parameter(2))  flowFilter.setParameter('gamma', ll.Parameter(0.0005))  flowFilter.setParameter('gamma_low', ll.Parameter(0.0005))   # use selected floating point precision  flowFilter.setParameter('float_precision', ll.Parameter(precision.value))   flowFilter.bind('in_gray', RGBA2Gray.getPort('out_gray'))  flowFilter.init()   duration = session.createDuration()   cmdBuffer = session.createCommandBuffer()  cmdBuffer.begin()  cmdBuffer.run(RGBA2Gray)  cmdBuffer.memoryBarrier()   # probe the runtime of the flowfilter node  cmdBuffer.durationStart(duration)  cmdBuffer.run(flowFilter)  cmdBuffer.memoryBarrier()  cmdBuffer.durationEnd(duration)   cmdBuffer.end()   # run the command buffer N times and collect the runtime of the flow algorithm  for n in range(N):  session.run(cmdBuffer)  runtimeMilliseconds[precision][n] = duration.nanoseconds / 1e6    Here, the ll.FloatPrecision.FP16, ll.FloatPrecision.FP32 are new enum values for representing 16-bit and 32-bit floating point precision, respectively. The line flowFilter.setParameter('float_precision', ll.Parameter(precision.value)) configures the node with the given precision. Internally, the float_precision is used to instantiate any floating point image with the requested precision.\nNote: By convention, any node that allows selecting floating point precision will define the float_precision parameter and will expect one of the ll.FloatPrecision enum values.  The figure below shows the collected runtime for both floating point precisions. The median runtime for FP16 is 0.501ms, while for FP32 is 0.770ms. That is, the FP16 algorithm improves the runtime by 35% compared to FP32.\nOptical flow filter runtime using FP16 and FP32 floating point precision. Results collected on a Nvidia GTX-1080 (driver 460.91.03) running Ubuntu 20.04.   Modifications to GLSL shader code In terms of GLSL shader code, there are no changes to support FP16 or FP32 images. However, it is important to understand the underlying functioning. For instance, consider the GLSL implementation of the RGBA2HSVA compute node. Notice that the out_hsva port is bound to the shader as a rgba32f image:\nGLSL   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  #version 450  #include \u003clluvia/core.glsl\u003e #include \u003clluvia/core/color.glsl\u003e  layout(binding = 0, rgba8ui) uniform uimage2D in_rgba; layout(binding = 1, rgba32f) uniform writeonly image2D out_hsva;  layout(push_constant) uniform const_0 {  float min_chroma; } params;  void main() {   const float min_chroma = params.min_chroma;   const ivec2 coords = LL_GLOBAL_COORDS_2D;  const ivec2 imgSize = imageSize(out_hsva);   if (coords.x \u003e imgSize.x || coords.y \u003e imgSize.y) {  return;  }   const uvec4 RGBA = imageLoad(in_rgba, coords);  const vec4 HSVA = color_rgba2hsva(RGBA, min_chroma);   imageStore(out_hsva, coords, HSVA); }    Images compatible with the rgba32f image format can be bound as output. The shader image load store extension defines the compatibility rules to be able to bind images to shaders. For this case in particular, it is possible to bind either a rgba16f or rgba32f images to the output. The shader will execute all arithmetic operations using 32-bit floating point precision. When storing an image texel using imageStore(out_hsva, coords, HSVA), the shader will reinterpret the vec4 HSVA either as a 16 or 32-bit floating vector, according to the image bound to out_hsva.\nThe shader image load store extension describes the way texels are re-interepret during load/store operations.  In terms of Lua code to build the node, these are the considerations to support different precisions:\n Define the float_precision parameter with default value to ll.FloatPrecision.FP32. Allocate the node objects according to the selected precision.  In the code below, line local outImageChannelType = ll.floatPrecisionToImageChannelType(float_precision) transforms the recevied ll.FloatPrecision value to the corresponding ll.ChannelType. Then, out_hsva is created and bound to the node.\nLua   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  local builder = ll.class(ll.ComputeNodeBuilder)  builder.name = 'lluvia/color/RGBA2HSVA'  function builder.newDescriptor()   local desc = ll.ComputeNodeDescriptor.new()  desc:init(builder.name, ll.ComputeDimension.D2)   -- define the float_precision parameter with default value  desc:setParameter('float_precision', ll.FloatPrecision.FP32)   local in_rgba = ll.PortDescriptor.new(0, 'in_rgba', ll.PortDirection.In, ll.PortType.ImageView)  in_rgba:checkImageChannelCountIs(ll.ChannelCount.C4)  in_rgba:checkImageChannelTypeIs(ll.ChannelType.Uint8)   desc:addPort(in_rgba)  desc:addPort(ll.PortDescriptor.new(1, 'out_hsva', ll.PortDirection.Out, ll.PortType.ImageView))   return desc end  function builder.onNodeInit(node)   local in_rgba = node:getPort('in_rgba')   -- receive the selected float_precision  local float_precision = node:getParameter('float_precision')   -- transform float precision to a suitable image channel type  local outImageChannelType = ll.floatPrecisionToImageChannelType(float_precision)   -------------------------------------------------------  -- allocate out_hsva  -------------------------------------------------------  local imgDesc = ll.ImageDescriptor.new()  imgDesc.width = in_rgba.width  imgDesc.height = in_rgba.height  imgDesc.depth = in_rgba.depth  imgDesc.channelCount = ll.ChannelCount.C4  imgDesc.channelType = outImageChannelType   local imgViewDesc = ll.ImageViewDescriptor.new()  imgViewDesc.filterMode = ll.ImageFilterMode.Nearest  imgViewDesc.normalizedCoordinates = false  imgViewDesc.isSampled = false  imgViewDesc:setAddressMode(ll.ImageAddressMode.Repeat)   -- ll::Memory where out_hsva will be allocated  local memory = in_rgba.memory  local out_hsva = memory:createImageView(imgDesc, imgViewDesc)   -- need to change image layout before binding  out_hsva:changeImageLayout(ll.ImageLayout.General)   node:bind('out_hsva', out_hsva)  node:configureGridShape(ll.vec3ui.new(out_hsva.width, out_hsva.height, 1)) end  -- register builder in the system ll.registerNodeBuilder(builder)    Discussion There are several floating point precisions available to use in compute shaders: FP16, FP132, and FP64, are the ones more commonly available in commodity GPU hardware. The ability to control the underlying floating point precision used in compute pipelines can improve runtime performance, as the transfer rate of data from and to memory can increase. The choice of a given precision must be made carefully, as it might affect the accuracy of the algorithm.\nReferences  IEEE 754 standard Vulkan GPU Info page OpenGL shader image load-store extension  ","categories":"","description":"Discusses how to use different floating point precisions available in the GPU, and how to take advantage of smaller representations to improve runtime performance.","excerpt":"Discusses how to use different floating point precisions available in …","ref":"/blog/2022/02/12/working-with-floating-point-precision/","tags":"","title":"Working with floating point precision"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/articles/","tags":"","title":"Articles"},{"body":"Converts a BGRA image to gray scale.\nThe gray scale value is computed as:\ngray = dot(RGBA, vec4(0.29899999, 0.58700001, 0.114, 0.0))  where RGBA is read and ordered from the in_bgra input image.\nInputs in_bgra : ImageView. rgba8ui image in BGRA channel order.\nOutputs out_gray : ImageView r8ui image. Thre gray scale values are in the range [0, 255]. This image is allocated in the same memory as in_bgra.\n","categories":"","description":"Converts a BGRA image to gray scale.","excerpt":"Converts a BGRA image to gray scale.","ref":"/nodes/lluvia/color/bgra2gray/","tags":"","title":"BGRA2Gray"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/blog/","tags":"","title":"Docsy Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/camera/","tags":"","title":"camera"},{"body":"Rectifies an RGBA input image applying camera distortion model.\nStandard distortion model For the standard camera model, the undistortion process is as follows. A 3D point $\\mathbf{x} \\in \\mathbb{R}^3$ is expressed relative to the camera body fixed frame. It projects onto the camera image plane as pixel $\\mathbf{p} := (u, v)^\\top \\in \\mathbb{R}^2$ as\n$$ \\begin{equation} \\begin{pmatrix} \\mathbf{p} \\\\ 1 \\end{pmatrix} := \\begin{pmatrix} u \\\\ v \\\\ 1 \\end{pmatrix} = \\frac{\\mathbf{K} \\mathbf{x}}{ \\left\u003c e_3, \\mathbf{x} \\right\u003e} \\end{equation} $$\nwhere $\\mathbf{K} \\in \\mathbb{R}^{3\\times3}$ is the camera intrinsics matrix, $e_3 := (0, 0, 1)^\\top$, and $\\left\u003c e_3, \\mathbf{x} \\right\u003e$ is the dot product between the two vectors. The units of $\\mathbf{p}$ are actual pixel coordinates in the ranges $u \\in [0, W)$ and $v \\in [0, H)$, with $W$ and $H$ denoting the image width and height respectively.\nGiven a pixel point, the corresponding 3D coordinate $\\bar{\\mathbf{x}}$ in the image plane is defined as:\n$$ \\begin{equation} \\bar{\\mathbf{x}} := \\begin{pmatrix} \\bar{x} \\\\ \\bar{y} \\\\ \\bar{z} \\\\ \\end{pmatrix} = \\mathbf{K}^{-1} \\begin{pmatrix} \\mathbf{p} \\\\ 1 \\end{pmatrix} \\end{equation} $$\nThe standard distortion model is formed by two components:\n A radial component parameterized by three coefficients: $k_1$, $k_2$, and $k_3$. A tangential component with two parameters: $p_1$ and $p_2$.  The radial distortion component for a given pixel $\\mathbf{p}$ is computed as\n$$ \\begin{equation} \\bar{\\mathbf{x}}_r := R \\begin{pmatrix} \\bar{x} \\\\ \\bar{y} \\\\ 0 \\end{pmatrix} \\end{equation} $$\nwhere $R \\in \\mathbb{R}$ is\n$$ \\begin{equation} R = k_1 r^2 + k_2 r^4 + k_3 r^6 \\end{equation} $$\nwith\n$$ \\begin{equation} r^2 = \\bar{x}^2 + \\bar{y}^2 \\end{equation} $$\nand $\\bar{x}, \\bar{y}$ are the $x$ and $y$ coordinates of the projection of pixel $\\mathbf{p}$ using equation (2).\nThe tangential distortion is computed as:\n$$ \\begin{equation} \\bar{\\mathbf{x}}_p := \\begin{pmatrix} 2 p_1 \\bar{x}\\bar{y} + p_2(r^2 + 2\\bar{x}^2) \\\\ p_1(r^2 + 2 \\bar{y}^2) + 2 p_2 \\bar{x}\\bar{y} \\\\ 0 \\end{pmatrix} \\end{equation} $$\nFinally, the undistorted image plane coordinates $\\bar{\\mathbf{x}}_u$ is computed as:\n$$ \\begin{equation} \\bar{\\mathbf{x}}_u = \\bar{\\mathbf{x}} + \\bar{\\mathbf{x}}_r + \\bar{\\mathbf{x}}_p \\end{equation} $$\nGiven $\\bar{\\mathbf{x}}_u$, the corresponding undistorted pixel coordinate is:\n$$ \\begin{equation} \\begin{pmatrix} \\mathbf{p}_u \\\\ 1 \\end{pmatrix} := \\begin{pmatrix} u_u \\\\ v_u \\\\ 1 \\end{pmatrix} = \\frac{\\mathbf{K} \\bar{\\mathbf{x}}_u}{ \\left\u003c e_3, \\bar{\\mathbf{x}}_u \\right\u003e} \\end{equation} $$\nParameters camera_model : int. Defaults to 0. The camera model used for rectifying the image. Possible values are:\n* 0: standard model  Inputs in_rgba : ImageView. rgba8ui image.\nin_camera : UniformBuffer. Uniform buffer containing the camera data. The buffer is interpreted as GLSL ll_camera struct. The alignment of the struct follows the GLSL std140 rules of alignment.\n1 2 3 4 5 6  struct ll_camera {  mat3 K;  mat3 Kinv;  vec4 radialDistortion;  vec4 tangentialDistortion; };   Outputs out_rgba : ImageView rgba8ui image. The rectified image.\nExamples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  import lluvia as ll import lluvia.util as ll_util import numpy as np import matplotlib.pyplot as plt  session = ll.createSession()  # memory to store the input and output images memory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal)  # memory to store the uniform buffer with the camera parameters host_memory = session.createMemory([ll.MemoryPropertyFlagBits.DeviceLocal,  ll.MemoryPropertyFlagBits.HostVisible,  ll.MemoryPropertyFlagBits.HostCoherent])  # read a sample image sampleImage = ll_util.readSampleImage('koala')  # draw a grid on top of the sample image Yrange = np.arange(0, sampleImage.shape[0], 128) Ylines = np.concatenate([n + Yrange for n in range(4)])  Xrange = np.arange(0, sampleImage.shape[1], 128) Xlines = np.concatenate([n + Xrange for n in range(4)])  sampleImage[Ylines, ...] = 0 sampleImage[:, Xlines, ...] = 0  # the input image view must be sampled. This example uses nearest neighbor interpolation in_rgba = memory.createImageViewFromHost(sampleImage,  filterMode=ll.ImageFilterMode.Nearest,  addressMode=ll.ImageAddressMode.Repeat,  normalizedCoordinates=False,  sampled=True)  ################################################### # Camera parameters W = float(in_rgba.width) H = float(in_rgba.height)  # Dummy camera matrix K = np.array([[W, 0, 0.5*(W -1)],  [0, H, 0.5*(H -1)],  [0, 0, 1] ], dtype=np.float32, order='F') Kinv = np.linalg.inv(K) radialDistortion = np.array([0.5, 0, 0, 0,], dtype=np.float32) tangentialDistortion = np.array([0.1, 0, 0, 0], dtype=np.float32)  # align the matrices according to the STD140 rules (column major, 4-component vectors) K_aligned = np.zeros((4,3), dtype=np.float32, order='F'); K_aligned[:3, :3] = K Kinv_aligned = np.zeros((4,3), dtype=np.float32, order='F'); Kinv_aligned[:3, :3] = Kinv  # create bytes buffer from matrices buf = K_aligned.tobytes(order='F') + Kinv_aligned.tobytes(order='F') + radialDistortion.tobytes() + tangentialDistortion.tobytes() npBuf = np.frombuffer(buf, dtype=np.uint8)  # in_camera uniform buffer in_camera = host_memory.createBufferFromHost(npBuf, usageFlags=[ll.BufferUsageFlagBits.TransferSrc,  ll.BufferUsageFlagBits.TransferDst,  ll.BufferUsageFlagBits.UniformBuffer])  ################################################### # Compute node CameraUndistort = session.createComputeNode('lluvia/camera/CameraUndistort_rgba8ui') CameraUndistort.setParameter('camera_model', ll.Parameter(1)) # standard model CameraUndistort.bind('in_rgba', in_rgba) CameraUndistort.bind('in_camera', in_camera) CameraUndistort.init()  CameraUndistort.run()  out_rgba = CameraUndistort.getPort('out_rgba')  fig = plt.figure(figsize=(30, 15)); fig.set_tight_layout(True) plt.subplot2grid((1,2), (0,0)); plt.imshow(in_rgba.toHost()[..., :3]) plt.subplot2grid((1,2), (0,1)); plt.imshow(out_rgba.toHost()[..., :3]) plt.show()   References  Zhang, Z., 2000. A flexible new technique for camera calibration. IEEE Transactions on pattern analysis and machine intelligence, 22(11), pp.1330-1334. Wei, G.Q. and De Ma, S., 1994. Implicit and explicit camera calibration: Theory and experiments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5), pp.469-480. Szeliski, R., 2010. Computer vision: algorithms and applications. Springer Science \u0026 Business Media. Hartley, R. and Zisserman, A., 2003. Multiple view geometry in computer vision. Cambridge university press.  ","categories":"","description":"Rectifies an RGBA input image applying camera distortion model.","excerpt":"Rectifies an RGBA input image applying camera distortion model.","ref":"/nodes/lluvia/camera/cameraundistort_rgba8ui/","tags":"","title":"CameraUndistort_rgba8ui"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/color/","tags":"","title":"color"},{"body":"Encodes a 2D optical flow field as RGBA color.\nThe encoding uses the HSV color wheel to encode angle, with some modifications:\n Saturation increases linearly with the norm Value increases logaritmically with the norm. For norm equal zero, the RGB value is black For vector norms greater than max_flow, set value to 0.8. Invalid pixels are coloured white.  Parameters max_flow : float. Defaults to 1.0. The maximum norm of any vector in the field.\nInputs in_flow : ImageView. {rg16f, rg32f} image. Input optical flow\nOutputs out_rgba : ImageView rgba8ui image. The encoded color of the optical flow field.\n","categories":"","description":"Encodes a 2D optical flow field as RGBA color.","excerpt":"Encodes a 2D optical flow field as RGBA color.","ref":"/nodes/lluvia/viz/flow2rgba/","tags":"","title":"Flow2RGBA"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/opticalflow/flowfilter/","tags":"","title":"flowfilter"},{"body":"An optical flow filter with pyramidal implementation.\nParameters max_flow : float. Defaults to 1.0 The max magnitude allowed for the optical flow output.\nlevels : int. Defaults to 1. The number of levels to create. If the value is 1, the in_gray input is bound as out_gray in the output, without any memory copy.\ngamma : float. Defaults to 0.01 The filter gain for the update step at the top level of the pyramid\ngamma_low : float. Defaults to 0.01 The filter gains for the update step at the intermediate pyramid levels.\nsmooth_iterations : int. Defaults to 1. The number of smooth iterations to apply to the estimated flow at each pyramid level.\nfloat_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray and out_flow will be of this floating point precision.\nInputs in_gray : ImageView r8ui image. The input gray-scale image.\nOutputs out_gray : ImageView {r16f, r32f} image. The gray-scale image after one iteration of the algorithm. The floating point precision of this output is affected by the float_precision parameter.\nout_flow : ImageView {rg16f, rg32f} image. The estimated optical flow. The floating point precision of this output is affected by the float_precision parameter.\n","categories":"","description":"An optical flow filter with pyramidal implementation.","excerpt":"An optical flow filter with pyramidal implementation.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowfilter/","tags":"","title":"FlowFilter"},{"body":"An intermediate pyramid-level filter for computing optical flow.\nThe in_gray input is the gray-scale image at this particular pyramid level. in_flow corresponds to the estimated flow from a higher pyramid level, and its resolution is half of that of in_gray.\nThe output out_flow is the estimated optical flow for this level. It refines the information in in_flow using higher resolution data from this level.\nParameters gamma : float. Defaults to 0.01 The filter gains for the update step.\nmax_flow : float. Defaults to 1.0 The max magnitude allowed for the optical flow output.\nsmooth_iterations : int. Defaults to 1. The number of smooth iterations to apply to the estimated flow.\nfloat_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray, out_flow and out_delfa_flow will be of this floating point precision.\nInputs in_gray : ImageView r8ui image. The input gray-scale image.\nin_flow : ImageView {rg16f, rg32f} image. The input optical flow.\nOutputs out_gray : ImageView r32f image. The gray-scale image after one iteration of the algorithm.\nout_flow : ImageView rg32f image. The estimated optical flow.\nout_delta_flow : ImageView rg32f image. The estimated optical flow.\n","categories":"","description":"An intermediate pyramid-level filter for computing optical flow.","excerpt":"An intermediate pyramid-level filter for computing optical flow.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowfilterdelta/","tags":"","title":"FlowFilterDelta"},{"body":"A single pyramid level version of the filtering algorithm for computing optical flow.\nThis node can be used as a stand-alone algorithm when the optical flow magnitude is expected to be small (~2pix). Otherwise, it is recommended to use the FlowFilter node for a pyramidal version of the algorithm.\nParameters gamma : float. Defaults to 0.01 The filter gains for the update step.\nmax_flow : float. Defaults to 1.0 The max magnitude allowed for the optical flow output.\nsmooth_iterations : int. Defaults to 1. The number of smooth iterations to apply to the estimated flow.\nfloat_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray and out_flow will be of this floating point precision.\nInputs in_gray : ImageView r8ui image. The input gray-scale image.\nOutputs out_gray : ImageView {r16f, r32f} image. The gray-scale image after one iteration of the algorithm. The image format depends on the float_precision parameter.\nout_flow : ImageView {rg16f, rg32f} image. The estimated optical flow. The image format depends on the float_precision parameter.\n","categories":"","description":"A single pyramid level version of the filtering algorithm for computing optical flow.","excerpt":"A single pyramid level version of the filtering algorithm for …","ref":"/nodes/lluvia/opticalflow/flowfilter/flowfiltersimple/","tags":"","title":"FlowFilterSimple"},{"body":"Computes a forward prediction of an optical flow field.\nThe prediction is computed as a series of X and Y predictions using the FlowPredictX and FlowPredictY compute nodes.\nBased on the value of max_flow parameter, the time step dt is calculated as:\ndt = 1.0 / ceil(max_flow)\nParameters max_flow : float. Defaults to 1.0 The max magnitude of any flow vector in in_flow field.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The image format will be the same as in_flow.\n","categories":"","description":"Computes a forward prediction of an optical flow field.","excerpt":"Computes a forward prediction of an optical flow field.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredict/","tags":"","title":"FlowPredict"},{"body":"Computes a forward prediction of an optical flow field and payloads.\nThe prediction is computed as a series of X and Y predictions using the FlowPredictX and FlowPredictY compute nodes.\nBased on the value of max_flow parameter, the time step dt is calculated as:\ndt = 1.0 / ceil(max_flow)\nParameters max_flow : float. Defaults to 1.0 The max magnitude of any flow vector in in_flow field.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nin_gray : ImageView {r16f, r32f} image. Input scalar field payload.\nin_vector : Imageview {rg16f, rg32f} image. Input vector field payload.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The image format will be the same as in_flow.\nout_gray : ImageView {r16f, r32f} image. Predicted scalar field. The image format will be the same as in_gray.\nout_vector : ImageView {rg16f, rg32f} image. Predicted vector field. The image format will be the same as in_vector.\n","categories":"","description":"Computes a forward prediction of an optical flow field and payloads.","excerpt":"Computes a forward prediction of an optical flow field and payloads.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredictpayload/","tags":"","title":"FlowPredictPayload"},{"body":"Computes one step predition of the optical flow and payloads along the X axis.\nParameters dt : float. Defaults to 1.0 The time step used for the prediction.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nin_gray : ImageView {r16f, r32f} image. Input scalar field payload.\nin_vector : Imageview {rg16f, rg32f} image. Input vector field payload.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The image format will be the same as in_flow.\nout_gray : ImageView {r16f, r32f} image. Predicted scalar field. The image format will be the same as in_gray.\nout_vector : ImageView {rg16f, rg32f} image. Predicted vector field. The image format will be the same as in_vector.\n","categories":"","description":"Computes one step predition of the optical flow and payloads along the X axis.","excerpt":"Computes one step predition of the optical flow and payloads along the …","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredictpayloadx/","tags":"","title":"FlowPredictPayloadX"},{"body":"Computes one step predition of the optical flow and payloads along the Y axis.\nParameters dt : float. Defaults to 1.0 The time step used for the prediction.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nin_gray : ImageView {r16f, r32f} image. Input scalar field payload.\nin_vector : Imageview {rg16f, rg32f} image. Input vector field payload.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The image format will be the same as in_flow.\nout_gray : ImageView {r16f, r32f} image. Predicted scalar field. The image format will be the same as in_gray.\nout_vector : ImageView {rg16f, rg32f} image. Predicted vector field. The image format will be the same as in_vector.\n","categories":"","description":"Computes one step predition of the optical flow and payloads along the Y axis.","excerpt":"Computes one step predition of the optical flow and payloads along the …","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredictpayloady/","tags":"","title":"FlowPredictPayloadY"},{"body":"Computes one step predition of the optical flow along the X axis.\nParameters dt : float. Defaults to 1.0 The time step used for the prediction.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The floating point precision will be the same as in_flow.\n","categories":"","description":"Computes one step predition of the optical flow along the X axis.","excerpt":"Computes one step predition of the optical flow along the X axis.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredictx/","tags":"","title":"FlowPredictX"},{"body":"Computes one step predition of the optical flow along the Y axis.\nParameters dt : float. Defaults to 1.0 The time step used for the prediction.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow used for the prediction.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Predicted optical flow. The floating point precision will be the same as in_flow.\n","categories":"","description":"Computes one step predition of the optical flow along the Y axis.","excerpt":"Computes one step predition of the optical flow along the Y axis.","ref":"/nodes/lluvia/opticalflow/flowfilter/flowpredicty/","tags":"","title":"FlowPredictY"},{"body":"Smooths an optical flow field using a 3x3 Gaussian filter\nParameters allocate_output : int. Defaults to 1. Whether or not the out_flow output should be allocated. If zero, this node does not allocate any output and expects out_flow to be allocated and bound before this node is initialized.\nInputs in_flow : ImageView {rg16f, rg32f} image. The input optical flow.\nOutputs out_flow : ImageView {rg16f, rg32f} image. Smoothed optical flow. The floating point precision will be the same as in_flow.\n","categories":"","description":"Smooths an optical flow field using a 3x3 Gaussian filter","excerpt":"Smooths an optical flow field using a 3x3 Gaussian filter","ref":"/nodes/lluvia/opticalflow/flowfilter/flowsmooth/","tags":"","title":"FlowSmooth"},{"body":"Computes one step update of the optical flow based on image data and previous estimate.\nParameters gamma : float. Defaults to 0.01 The filter gains for the update step.\nmax_flow : float. Defaults to 1.0 The max magnitude allowed for the optical flow output.\nInputs in_gray : ImageView {r16f, r32f} image. The input gray-scale image at current time step.\nin_gradient : ImageView {rg16f, rg32f} image. The input gray-scale image gradient at current time step.\nin_flow : Imageview {rg16f, rg32f} image. The current estimation of the optical flow.\nOutputs out_gray : ImageView {r16f, r32f} image. The image estimate for the next time step. It is a memory copy of in_gray.\nout_flow : ImageView {rg16f, rg32f} image. The updated optical flow. This image is allocated externally in FlowFilterSimple and bound to this node. This way, the loop between FlowPredict and FlowUpdate can be broken.\n","categories":"","description":"Computes one step update of the optical flow based on image data and previous estimate.","excerpt":"Computes one step update of the optical flow based on image data and …","ref":"/nodes/lluvia/opticalflow/flowfilter/flowupdate/","tags":"","title":"FlowUpdate"},{"body":"Computes one step update of the optical flow based on image data and previous estimate.\nParameters gamma : float. Defaults to 0.01 The filter gains for the update step.\nmax_flow : float. Defaults to 1.0 The max magnitude allowed for the optical flow output.\nInputs in_gray : ImageView {r16f, r32f} image. The input gray-scale image at current time step.\nin_gradient : ImageView {rg16f, rg32f} image. The input gray-scale image gradient at current time step.\nin_delta_flow : Imageview {rg16f, rg32f} image. The current estimation of the optical flow.\nin_gray_old : ImageView {r16f, r32f} image. The input gray-scale image at current time step.\nin_flow : SampledImageview {rg16f, rg32f} image. The current estimation of the optical flow from a level above in the pyramid. This image view has half the resolution of the other inputs.\nOutputs out_gray : ImageView {r16f, r32f} image. The image estimate for the next time step. It is a memory copy of in_gray. This image is allocated externally in FlowFilterDelta and bound to this node. This way, the loop between FlowPredictPayload and FlowUpdateDelta can be broken.\nout_flow : ImageView {rg16f, rg32f} image. The updated optical flow. The floating point precision of this port depends on the precision of in_flow.\nout_delta_flow : ImageView {rg16f, rg32f} image. The updated optical flow. This image is allocated externally in FlowFilterDelta and bound to this node. This way, the loop between FlowPredictPayload and FlowUpdateDelta can be broken.\n","categories":"","description":"Computes one step update of the optical flow based on image data and previous estimate.","excerpt":"Computes one step update of the optical flow based on image data and …","ref":"/nodes/lluvia/opticalflow/flowfilter/flowupdatedelta/","tags":"","title":"FlowUpdateDelta"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/opticalflow/hornschunck/","tags":"","title":"HornSchunck"},{"body":"Horn and Schunck variational optical flow algorithm.\nParameters alpha : float. Defaults to 0.05. Regularization gain.\niterations : int. Defaults to 1. Number of iterations run to compute the optical flow.\nfloat_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray and out_flow will be of this floating point precision.\nInputs in_gray : ImageView r8ui image. The input gray-scale image.\nOutputs out_gray : ImageView {r16f, r32f} image. The gray-scale image after one iteration of the algorithm. The image format depends on the float_precision parameter.\nout_flow : ImageView {rg16f, rg32f} image. The estimated optical flow. The image format depends on the float_precision parameter.\nExamples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  import lluvia as ll import lluvia.util as ll_util import matplotlib.pyplot as plt  # read two images as numpy arrays frame_0 = ll_util.readRGBA('path to first image...') frame_1 = ll_util.readRGBA('path to second image...')  # global session and memory objects session = ll.createSession() memory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal)  # this is the input of the comple pipeline in_rgba = memory.createImageViewFromHost(frame_0)  RGBA2Gray = session.createComputeNode('lluvia/color/RGBA2Gray') RGBA2Gray.bind('in_rgba', in_rgba) RGBA2Gray.init() RGBA2Gray.run() # run the node immediately in order to populate out_gray with valid values  in_gray = RGBA2Gray.getPort('out_gray')  HornSchunck = session.createContainerNode('lluvia/opticalflow/HornSchunck/HornSchunck') HornSchunck.setParameter('alpha', ll.Parameter(0.05)) HornSchunck.setParameter('iterations', ll.Parameter(1000)) HornSchunck.setParameter('float_precision', ll.Parameter(ll.FloatPrecision.FP32.value)) HornSchunck.bind('in_gray', in_gray)  # when the node is initialized, it transfers the content of in_gray to out_gray. HornSchunck.init()  out_gray = HornSchunck.getPort('out_gray') out_flow = HornSchunck.getPort('out_flow')  flow2RGBA = session.createComputeNode('lluvia/viz/Flow2RGBA') flow2RGBA.setParameter('max_flow', ll.Parameter(float(2))) flow2RGBA.bind('in_flow', out_flow) flow2RGBA.init()  out_flow_rgba = flow2RGBA.getPort('out_rgba')  duration = session.createDuration()  cmdBuffer = session.createCommandBuffer() cmdBuffer.begin() cmdBuffer.run(RGBA2Gray) cmdBuffer.memoryBarrier() cmdBuffer.durationStart(duration) cmdBuffer.run(HornSchunck) cmdBuffer.memoryBarrier() cmdBuffer.durationEnd(duration) cmdBuffer.run(flow2RGBA) cmdBuffer.end()  # copy the content of the second frame to the in_rgba image before running the whole pipeline in_rgba.fromHost(frame_1)  # run the pipeline session.run(cmdBuffer)  # print runtime in milliseconds print('{0:.02f}ms'.format(duration.nanoseconds / 1e6))  fig = plt.figure(figsize=(10, 6)); fig.set_tight_layout(True) plt.subplot2grid((1,2), (0, 0)); plt.imshow(out_gray.toHost(), vmin=0, vmax=1, cmap='gray') plt.subplot2grid((1,2), (0, 1)); plt.imshow(out_flow_rgba.toHost()) plt.show()   ","categories":"","description":"Horn and Schunck variational optical flow algorithm.","excerpt":"Horn and Schunck variational optical flow algorithm.","ref":"/nodes/lluvia/opticalflow/hornschunck/hornschunck/","tags":"","title":"HornSchunck"},{"body":"Converts a HSV image to RGB color space.\nThe conversion follows the formulae presented in https://en.wikipedia.org/wiki/HSL_and_HSV.\nInputs in_hsva : ImageView. {rgba16f, rgba32f} image. The color componens must lie within the following ranges:\n* H in [0, 2*pi] * S in [0, 1] * V in [0, 1] * A in [0, 1]  Outputs out_rgba : ImageView rgba8ui image. This image is allocated in the same memory as in_hsva.\n","categories":"","description":"Converts a HSV image to RGB color space.","excerpt":"Converts a HSV image to RGB color space.","ref":"/nodes/lluvia/color/hsva2rgba/","tags":"","title":"HSVA2RGBA"},{"body":"Downsamples a gray level image along the X axis.\nLet W and H denote the width and height of the input in_gray image, respectively. The shape of the out_gray image is:\n out_gray.W = floor(W / 2) out_gray.H = H  That is, the output width is always an even number.\nThe value of a pixel out_gray(x, y) is computed as:\nout_gray(x, y) = 0.25in_gray(2x - 1, y) + 0.5in_gray(2x, y) + 0.25in_gray(2x + 1, y)\nInputs in_gray : ImageView. r8ui image.\nOutputs out_gray : ImageView r8ui image. The downsampled image.\n","categories":"","description":"Downsamples a gray level image along the X axis.","excerpt":"Downsamples a gray level image along the X axis.","ref":"/nodes/lluvia/imgproc/imagedownsamplex_r8ui/","tags":"","title":"ImageDownsampleX_r8ui"},{"body":"Downsamples a gray level image along the Y axis.\nLet W and H denote the width and height of the input in_gray image, respectively. The shape of the out_gray image is:\n out_gray.W = W out_gray.H = floor(H / 2)  That is, the output height is always an even number.\nThe value of a pixel out_gray(x, y) is computed as:\nout_gray(x, y) = 0.25in_gray(x, 2y - 1) + 0.5in_gray(x, 2y) + 0.25in_gray(x, 2y + 1)\nInputs in_gray : ImageView. r8ui image.\nOutputs out_gray : ImageView r8ui image. The downsampled image.\n","categories":"","description":"Downsamples a gray level image along the Y axis.","excerpt":"Downsamples a gray level image along the Y axis.","ref":"/nodes/lluvia/imgproc/imagedownsampley_r8ui/","tags":"","title":"ImageDownsampleY_r8ui"},{"body":"Computes the image model from an in_gray image.\nThe outputs are a low-pass filtered version of in_gray using a 3x3 Gaussian filter, and a 2D image representing the XY partial differences of the low-pass image.\nParameters float_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The outputs out_gray and out_gradient will be of this floating point precision.\nInputs in_gray : ImageView. r8ui image.\nOutputs out_gray : ImageView {r16f, r32f} image. The low-pass filtered version of in_gray. The values are normalized to the range [0, 1]\nout_gradient: ImageView {rg16f, rg32f} image. The X and Y gradient components of in_gray. The values are normalized to the range [-1, 1]\n","categories":"","description":"Computes the image model from an in_gray image.","excerpt":"Computes the image model from an in_gray image.","ref":"/nodes/lluvia/opticalflow/flowfilter/imagemodel/","tags":"","title":"ImageModel"},{"body":"Normalizes the content of a 1-channel unsigned integer image and stores it into a 1-channel floating point image.\nParameters max_value : float. Defaults to 0.0. If the value is greater than 0. The input value is divided by max_value and then stored in out_image_float. Otherwise, the input value is casted to floating point and stored directly.\nInputs in_image_uint : ImageView. {r8ui, r16ui, r32ui} image.\nOutputs out_image_float : ImageView. {r16f, r32f} image. This image must be allocated externally.\nExamples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  import lluvia as ll import lluvia.util as ll_util import matplotlib.pyplot as plt  # global session and memory objects session = ll.createSession() memory = session.createMemory(ll.MemoryPropertyFlagBits.DeviceLocal)  # read a sample image sampleImage = ll_util.readSampleImage('mouse')  # this is the input of the comple pipeline in_rgba = memory.createImageViewFromHost(sampleImage)  RGBA2Gray = session.createComputeNode('lluvia/color/RGBA2Gray') RGBA2Gray.bind('in_rgba', in_rgba) RGBA2Gray.init()  # input and output to ImageNormalize. out_gray_float must be allocated outside of the node out_gray_uint = RGBA2Gray.getPort('out_gray') out_gray_float = memory.createImage(out_gray_uint.shape, ll.ChannelType.Float32).createImageView()  ImageNormalize = session.createComputeNode('lluvia/math/normalize/ImageNormalize_uint_C1') ImageNormalize.setParameter('max_value', ll.Parameter(255)) ImageNormalize.bind('in_image_uint', out_gray_uint) ImageNormalize.bind('out_image_float', out_gray_float) ImageNormalize.init()  duration = session.createDuration()  cmdBuffer = session.createCommandBuffer() cmdBuffer.begin() cmdBuffer.run(RGBA2Gray) cmdBuffer.memoryBarrier() cmdBuffer.durationStart(duration) cmdBuffer.run(ImageNormalize) cmdBuffer.memoryBarrier() cmdBuffer.durationEnd(duration) cmdBuffer.end()  # run the pipeline session.run(cmdBuffer)  # print runtime in milliseconds print('{0:.04f}ms'.format(duration.nanoseconds / 1e6))  fig = plt.figure(figsize=(30, 6)); fig.set_tight_layout(True) plt.subplot2grid((1, 3), (0, 0)); plt.imshow(in_rgba.toHost()[...,:3]) plt.subplot2grid((1, 3), (0, 1)); plt.imshow(out_gray_uint.toHost(), vmin=0, vmax=255); plt.colorbar() plt.subplot2grid((1, 3), (0, 2)); plt.imshow(out_gray_float.toHost(), vmin=0, vmax=1); plt.colorbar() plt.show()   ","categories":"","description":"Normalizes the content of a 1-channel unsigned integer image and stores it into a 1-channel floating point image.","excerpt":"Normalizes the content of a 1-channel unsigned integer image and …","ref":"/nodes/lluvia/math/normalize/imagenormalize_uint_c1/","tags":"","title":"ImageNormalize_uint_C1"},{"body":"Computes the parameters from the input image sequence.\nThe layout of the out_image_params vector is as follows:\n out_image_params.xy: The X and Y gradient components of in_gray. The values are normalized to the range [-1, 1] out_image_params.z: The time difference (in_gray - in_gray_old) out_image_params.w: The regularization value 1.0 / (alpha_square + grad_x*grad_x + grad_y*grad_y)  Parameters alpha : float. Defaults to 0.05. Regularization gain.\nfloat_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used accross the algorithm. The out_image_params output will be of this floating point precision.\nInputs in_gray : ImageView. r8ui image.\nin_gray_old: ImageView. {r16f, r32f} image. The low-pass filtered version of in_gray. The values are normalized to the range [0, 1].\nOutputs out_image_params : ImageView {rgba16f, rgba32f} image. Output image parameters\n","categories":"","description":"Computes the parameters from the input image sequence.","excerpt":"Computes the parameters from the input image sequence.","ref":"/nodes/lluvia/opticalflow/hornschunck/imageprocessor/","tags":"","title":"ImageProcessor"},{"body":"Creates an image pyramid from an input gray scale image.\nFor levels \u003e= 2, the pyramid is generated by concatenating a series of ImageDownsampleX_r8ui and ImageDownsampleY_r8ui nodes.\nParameters levels : int. Defaults to 1. The number of levels to create. If the value is 1, the in_gray input is bound as out_gray in the output, without any memory copy.\nInputs in_gray : ImageView. r8ui image.\nOutputs out_gray_0 : ImageView. r8ui image. The base level of the pyramid. This corresponds to in_gray. Other levels are named out_gray_1, to out_gray_{levels - 1}\nout_gray : ImageView r8ui image. The output at the top level of the pyramid. This is equivalent to out_gray_{levels - 1}\n","categories":"","description":"Creates an image pyramid from an input gray scale image.","excerpt":"Creates an image pyramid from an input gray scale image.","ref":"/nodes/lluvia/imgproc/imagepyramid_r8ui/","tags":"","title":"ImagePyramid_r8ui"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/imgproc/","tags":"","title":"imgproc"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/","tags":"","title":"lluvia"},{"body":" Welcome to Lluvia! Learn More   Download   A real-time compute engine for computer vision\n         Lluvia is a computer vision engine designed for real-time applications. Users can describe algorithms as a compute graph which is dispatched for execution on the GPU. Lluvia uses the Vulkan API, allowing it to run the same algorithm on different GPU platforms.\n      Get Started! See how you can use Lluvia for quick development of performant algorithms.\n Read more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\n Read more …\n    ","categories":"","description":"","excerpt":" Welcome to Lluvia! Learn More   Download   A real-time compute engine …","ref":"/","tags":"","title":"Lluvia"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/math/","tags":"","title":"math"},{"body":"Mediapipe is a cross-platform framework for creating complex Computer Vision and Deep Learning pipelines both for offline and streaming applications. It includes support to OpenCV and TensorFlow. By integrating Lluvia in mediapipe, it is possible to leverage its runtime capabilities as well as the interfacing with other popular frameworks.\nSetup Platform The following instructions are written for an Ubuntu host system.  Follow the linux instructions to install the basic dependencies to build lluvia in your host machine.\nMediapipe Clone the mediapipe repository in the same folder as lluvia.\n1 2  git clone https://github.com/google/mediapipe.git cd mediapipe   Setup Clang as default C++ compiler for mediapipe. Add the following line to mediapipe’s .bazelrc file\nbuild:linux --action_env=CC=clang Follow the installations instructions to configure OpenCV according to your installation. Also, enable GPU support. Once completed, run the hello_world application to check the build process:\n1 2 3 4  export GLOG_logtostderr=1  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/examples/desktop/hello_world:hello_world   the output should look like:\nI20221006 15:04:52.196460 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196496 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196501 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196537 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196563 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196588 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196615 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196640 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196666 12142 hello_world.cc:57] Hello World! I20221006 15:04:52.196691 12142 hello_world.cc:57] Hello World! Modifications to embed Lluvia as mediapipe’s dependency The next step is to include lluvia as a dependency of mediapipe. Append the configuration below to mediapipe’s WORKSPACE file to configure lluvia as a local_repository:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ########################################################### # LLUVIA ########################################################### local_repository(  name = \"lluvia\",  path = \"../lluvia\" # assuming lluvia was cloned in the same folder as mediapipe )  load(\"@lluvia//lluvia/bazel:workspace.bzl\", \"lluvia_workspace\") lluvia_workspace()  # Python configuration register_toolchains(\"@lluvia//platform:python_toolchain\")  load(\"@rules_python//python:pip.bzl\", \"pip_repositories\") pip_repositories()  # Platform configuration # Linux load(\"@lluvia//platform/linux:python.bzl\", \"python_linux\", \"numpy_linux\") python_linux(name = \"python_linux\") numpy_linux(name = \"numpy_linux\")  # Windows load(\"@lluvia//platform/windows:python.bzl\", \"python_windows\", \"numpy_windows\") python_windows(name = \"python_windows\") numpy_windows(name = \"numpy_windows\")  # Packaging rules load(\"@ll_rules_pkg//:deps.bzl\", \"rules_pkg_dependencies\") rules_pkg_dependencies()  # Vulkan rules load(\"@rules_vulkan//vulkan:repositories.bzl\", \"vulkan_repositories\") vulkan_repositories()  # Lua rules load(\"@rules_lua//toolchains:repositories.bzl\", \"lua_repositories\") lua_repositories()   Rerun mediapipe’s hello_world binary again to confirm the new workspace configuration works:\n1 2 3 4  export GLOG_logtostderr=1  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/examples/desktop/hello_world:hello_world   lluvia-mediapipe repository The lluvia-mediapipe project is an auxiliary repository containing the Calculators to interface with mediapipe. This repository needs to be cloned within mediapipe in order to consume its dependencies.\n1 2 3 4 5  # clone lluvia-mediapipe inside mediapipe  # assuming you are in the in the root folder were mediapipe was cloned cd mediapipe/mediapipe git clone https://github.com/jadarve/lluvia-mediapipe.git   The directory structure for all repositories should look like:\nlluvia \u003c-- lluvia repository mediapipe \u003c-- mediapipe repository ├── BUILD.bazel ├── docs ├── LICENSE ├── MANIFEST.in ├── mediapipe \u003c-- │ ├── BUILD │ ├── calculators │ ├── examples │ ├── framework │ ├── gpu │ ├── ... │ ├── lluvia-mediapipe \u003c-- lluvia-mediapipe repository ├── ... ├── .bazelrc └── WORKSPACE Next, run the lluvia_calculator_test target to verify the build and the runtime is correctly configured:\n1 2  bazel test --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/calculators:lluvia_calculator_test --test_output=all   LluviaCalculator The lluvia-mediapipe repository declares a new LluviaCalculator. This calculator is in charge of initializing Lluvia, binding input and output streams from mediapipe to lluvia ports, and running a given compute pipeline.\nThe figure below illustrates a basic mediapipe graph utilizing lluvia, while the code below shows the graph description using protobuffer text syntax\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  input_stream: \"input_stream\"output_stream: \"output_stream\"node: { calculator: \"LluviaCalculator\" input_stream: \"IN_0:input_stream\" output_stream: \"OUT_0:output_stream\" node_options { [type.googleapis.com/lluvia.LluviaCalculatorOptions]: { enable_debug: true library_path: \"path to .zip node library file\" script_path: \"path to .lua script defining the main container node\" container_node: \"mediapipe/examples/Passthrough\" input_port_binding: { mediapipe_tag: \"IN_0\" lluvia_port: \"in_image\" } } }}  where:\n The enable_debug flag tells whether or not the Vulkan debug extensions used by Lluvia should be loaded during session creation. This flag might be set to false in production applications to improve runtime performance. The library_path declare paths to node libraries (a .zip file) containing Lluvia nodes (Container and Compute). This attribute can be repeated several times. The script_path is the path to a lua script declaring a ContainerNode that Lluvia will instantiate as the “main” node to run inside the calculator. input_port_binding, maps mediapipe input tags to the main ContainerNode port. In the example above, mediapipe’s input tag IN_0 is mapped to lluvia’s in_image port.  Output stream mapping Currently there is no support for mapping outputs out of the calculator. The current convention is that the ContainerNode run in the LluviaCalculator produces an output_image port of type Imageview. The calculator maps that port to the OUT_0 output stream of the calculator.  Examples There are two example applications packed with lluvia-mediapipe\nsingle_image This app receives as command line arguments the path to an image file, a mediapipe graph definition and a lua script describing the ContainerNode to run inside of the LluviaCalculator.\nThe commands below execute the single_image app loading sample graphs packaged in the repository. The command assumes both lluvia and mediapipe repositories are cloned in the ${HOME}/git folder.\nPassthrough This graph simply copies the input image to the output, without any processing:\n1 2 3 4 5  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/single_image:single_image -- \\  --input_image=${HOME}/git/lluvia/lluvia/resources/mouse.jpg \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/passthrough/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/passthrough/graph.pbtxt      BGRA to Gray This graph runs the lluvia/color/BGRA2Gray compute node to convert from the BGRA input to gray scale:\n1 2 3 4 5  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/single_image:single_image -- \\  --input_image=${HOME}/git/lluvia/lluvia/resources/mouse.jpg \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/graph.pbtxt      webcam This application tries to open the default camera capture device in the host system using OpenCV VideoCapture class. Then it feeds the mediapipe graph the captured frames to be processed by the LluviaCalculator.\nBGRA to Gray 1 2 3 4  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/webcam:webcam -- \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/BGRA2Gray/graph.pbtxt   Horn and Schunck optical flow This is a more elaborate graph running inside the LluviaCalculator. First, the input image is converted from BGRA to gray scale, then is passes through the Horn and Schunck optical flow algorithm, and finally, the estimated optical flow is converted to RGBA (and then to BGRA) for visualization:\n@startuml skinparam linetype ortho state LluviaCalculator as \"LluviaCalculator\" { state input_stream as \"IN_0:input_stream\" \u003c\u003cinputPin\u003e\u003e state output_stream as \"OUT_0:output_stream\" \u003c\u003coutputPin\u003e\u003e state ContainerNode as \"mediapipe/examples/HornSchunck\" { state in_image \u003c\u003cinputPin\u003e\u003e state BGRA2Gray state HS as \"HornSchunck\" state Flow2RGBA state RGBA2BGRA input_stream -down-\u003e in_image in_image -down-\u003e BGRA2Gray BGRA2Gray -down-\u003e HS: in_gray HS -down-\u003e Flow2RGBA: in_flow Flow2RGBA -down-\u003e RGBA2BGRA: in_rgba RGBA2BGRA -down-\u003e out_image \u003c\u003coutputPin\u003e\u003e } out_image -down-\u003e output_stream \u003c\u003coutputPin\u003e\u003e } @enduml The command to run the example is:\n1 2 3 4  bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\  //mediapipe/lluvia-mediapipe/examples/desktop/webcam:webcam -- \\  --script_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/script.lua \\  --graph_file=${HOME}/git/mediapipe/mediapipe/lluvia-mediapipe/examples/desktop/graphs/HornSchunck/graph.pbtxt      References  Mediapipe lluvia-mediapipe  ","categories":"","description":"","excerpt":"Mediapipe is a cross-platform framework for creating complex Computer …","ref":"/docs/gettingstarted/mediapipe_integration/","tags":"","title":"Mediapipe integration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/math/normalize/","tags":"","title":"normalize"},{"body":"Computes one numeric iteration for computing the optical flow.\nInputs in_image_params : ImageView. {rgba16f, rgba32f} image. Image parameters computed by lluvia/opticalflow/HornSchunck/ImageProcessor\nin_flow: ImageView. {rg16f, rg32f} image. Optical flow from previous iteration.\nOutputs out_flow: ImageView. {rg16f, rg32f} image. Newly estimated flow. This output must be allocated outside of this node.\n","categories":"","description":"Computes one numeric iteration for computing the optical flow.","excerpt":"Computes one numeric iteration for computing the optical flow.","ref":"/nodes/lluvia/opticalflow/hornschunck/numericiteration/","tags":"","title":"NumericIteration"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/opticalflow/","tags":"","title":"opticalflow"},{"body":"Converts a RGBA image to BGRA.\nInputs in_rgba : ImageView. rgba8ui image in RGBA channel order.\nOutputs out_bgra : ImageView. rgba8ui image in BGRA channel order.\n","categories":"","description":"Converts a RGBA image to BGRA.","excerpt":"Converts a RGBA image to BGRA.","ref":"/nodes/lluvia/color/rgba2bgra/","tags":"","title":"RGBA2BGRA"},{"body":"Converts a RGB image to gray scale.\nThe gray scale value is computed as:\ngray = dot(RGBA, vec4(0.29899999, 0.58700001, 0.114, 0.0))  Inputs in_rgba : ImageView. rgba8ui image.\nOutputs out_gray : ImageView r8ui image. Thre gray scale values are in the range [0, 255]. This image is allocated in the same memory as in_rgba.\n","categories":"","description":"Converts a RGB image to gray scale.","excerpt":"Converts a RGB image to gray scale.","ref":"/nodes/lluvia/color/rgba2gray/","tags":"","title":"RGBA2Gray"},{"body":"Converts a RGB image to HSV color space.\nThe conversion follows the formulae presented in https://en.wikipedia.org/wiki/HSL_and_HSV.\nParameters min_chroma : float in [0, 1]. Defaults to 0.0. The minimum chromacity allowed in the conversion. If the chromacity of a given pixel is less than min_chroma, then the hue value is set to 0. float_precision : int. Defaults to ll.FloatPrecision.FP32. Floating point precision used to alloate out_hsva.\nInputs in_rgba : ImageView. rgba8ui image.\nOutputs out_hsva : ImageView {rgba16f, rgba32f} image. This image is allocated in the same memory as in_rgba.\nThe color componens lie within the following ranges: * H in [0, 2*pi] * S in [0, 1] * V in [0, 1] * A in [0, 1]  ","categories":"","description":"Converts a RGB image to HSV color space.","excerpt":"Converts a RGB image to HSV color space.","ref":"/nodes/lluvia/color/rgba2hsva/","tags":"","title":"RGBA2HSVA"},{"body":"Process a gray image with the Sobel gradient filter.\nThe in_gray image is filtered with the Sobel masks:\n [-1/4 0 1/4] Sx = [-1/2 0 1/2] [-1/4 0 1/4] [-1/4 -1/2 -1/4] Sy = [ 0 0 0 ] [1/4 1/2 1/4]  The result out_gradient is computed as:\nout_gradient.x = in_gray * Sx out_gradient.y = in_gray * Sy  The border is clamped to the edges of the image.\nInputs in_gray : ImageView r8ui image.\nOutputs out_gradient : ImageView rg32f image. The image gradient.\n","categories":"","description":"Process a gray image with the Sobel gradient filter.","excerpt":"Process a gray image with the Sobel gradient filter.","ref":"/nodes/lluvia/sobel/","tags":"","title":"Sobel"},{"body":"","categories":"","description":"","excerpt":"","ref":"/nodes/lluvia/viz/","tags":"","title":"viz"}]